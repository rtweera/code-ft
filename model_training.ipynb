{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtweera/code-ft/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    is_xla = True\n",
        "except:\n",
        "    print(\"TPU not found. `is_xla` set to `False`\")\n",
        "    is_xla = False"
      ],
      "metadata": {
        "id": "_98wkmGuL1CJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bcac18-f61c-4243-a5a7-88075ddfafd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU not found. `is_xla` set to `False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-aziTqauGvu7"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers peft accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import os\n",
        "\n",
        "if is_xla:\n",
        "  os.environ[\"XLA_USE_BF16\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize accelerator with bf16 (brain float 16)"
      ],
      "metadata": {
        "id": "caLPV7VgMvFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if is_xla:\n",
        "  from accelerate import Accelerator\n",
        "  accelerator = Accelerator(mixed_precision=\"bf16\")"
      ],
      "metadata": {
        "id": "UPR3-aZlMtNc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k_70N7POGvvE"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B\"  # Specify exact variant if needed (e.g., 7B, 1.5B)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get the list of ddirs in current director\n",
        "\n",
        "import os\n",
        "\n",
        "def list_directories(path='.'):\n",
        "  \"\"\"Lists the directories in the current directory.\"\"\"\n",
        "  directories = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "  return directories\n",
        "\n",
        "list_directories()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c8EGc0_J4dJ",
        "outputId": "663823ed-da5c-41d0-fda4-b55c0907a14e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WKaEOsbnGvvG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize datasets\n",
        "with open(\"train.txt\", \"r\") as f:\n",
        "    train_data = f.readlines()\n",
        "with open(\"val.txt\", \"r\") as f:\n",
        "    val_data = f.readlines()\n",
        "\n",
        "train_tokenized = tokenize_function(train_data)\n",
        "val_tokenized = tokenize_function(val_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aeaZl9pdGvvI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert to a format suitable for Trainer\n",
        "class SimpleDataset:\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = tokenized_data[\"input_ids\"]\n",
        "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.input_ids[idx], \"attention_mask\": self.attention_mask[idx]}\n",
        "\n",
        "train_dataset = SimpleDataset(train_tokenized)\n",
        "val_dataset = SimpleDataset(val_tokenized)\n",
        "\n",
        "# Data collator for CLM (shifts inputs to create targets)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ercAJ3tHGvvJ",
        "outputId": "db4e8551-ce8d-4887-d256-94de04a8ebc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,              # Rank of the adaptation matrices\n",
        "    lora_alpha=32,    # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers to adapt\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_qwen_ballerina\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=is_xla,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "if is_xla:\n",
        "  trainer = accelerator.prepare(trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qlff1TCUGvvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fc495d5-3e81-4aa2-8cee-dd2da7f50f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrtweera\u001b[0m (\u001b[33mrtw-rtweera\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_065555-bwdu714n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rtw-rtweera/huggingface/runs/bwdu714n' target=\"_blank\">./finetuned_qwen_ballerina</a></strong> to <a href='https://wandb.ai/rtw-rtweera/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rtw-rtweera/huggingface' target=\"_blank\">https://wandb.ai/rtw-rtweera/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rtw-rtweera/huggingface/runs/bwdu714n' target=\"_blank\">https://wandb.ai/rtw-rtweera/huggingface/runs/bwdu714n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2215' max='6642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2215/6642 54:10 < 1:48:21, 0.68 it/s, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.396600</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_determine_best_metric\u001b[0;34m(self, metrics, trial)\u001b[0m\n\u001b[1;32m   3150\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3151\u001b[0;31m                 \u001b[0mmetric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3152\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eval_loss'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e5e6b3b9574e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSaveStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_determine_best_metric\u001b[0;34m(self, metrics, trial)\u001b[0m\n\u001b[1;32m   3151\u001b[0m                 \u001b[0mmetric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3152\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3153\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   3154\u001b[0m                     \u001b[0;34mf\"The `metric_for_best_model` training argument is set to '{metric_to_check}', which is not found in the evaluation metrics. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m                     \u001b[0;34mf\"The available evaluation metrics are: {list(metrics.keys())}. Consider changing the `metric_for_best_model` via the TrainingArguments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual evaluation\n",
        "from transformers import Trainer\n",
        "temp_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "metrics = temp_trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "2mmq8ldLnWuB",
        "outputId": "42aaab7b-f43d-457e-dcd9-365f1f1ef742"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='567' max='567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [567/567 06:19]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_model_preparation_time': 0.0465, 'eval_runtime': 379.3769, 'eval_samples_per_second': 5.973, 'eval_steps_per_second': 1.495}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X2n5rMKIGvvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382a09d4-7277-405d-fd8c-f0a76ce455c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./finetuned_qwen_ballerina/tokenizer_config.json',\n",
              " './finetuned_qwen_ballerina/special_tokens_map.json',\n",
              " './finetuned_qwen_ballerina/vocab.json',\n",
              " './finetuned_qwen_ballerina/merges.txt',\n",
              " './finetuned_qwen_ballerina/added_tokens.json',\n",
              " './finetuned_qwen_ballerina/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./finetuned_qwen_ballerina\")\n",
        "tokenizer.save_pretrained(\"./finetuned_qwen_ballerina\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zip and save"
      ],
      "metadata": {
        "id": "kqMQOPCI9hs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: zip this file for downloading \"./finetuned_qwen_ballerina\"\n",
        "\n",
        "!zip -r /content/finetuned_qwen_ballerina.zip /content/finetuned_qwen_ballerina\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/finetuned_qwen_ballerina.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaMUqW03s__-",
        "outputId": "58864d4e-2f37-4929-9caa-f96dd4f63ffb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/finetuned_qwen_ballerina/ (stored 0%)\n",
            "  adding: content/finetuned_qwen_ballerina/merges.txt (deflated 57%)\n",
            "  adding: content/finetuned_qwen_ballerina/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/finetuned_qwen_ballerina/special_tokens_map.json (deflated 69%)\n",
            "  adding: content/finetuned_qwen_ballerina/vocab.json (deflated 61%)\n",
            "  adding: content/finetuned_qwen_ballerina/tokenizer_config.json (deflated 83%)\n",
            "  adding: content/finetuned_qwen_ballerina/added_tokens.json (deflated 67%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/ (stored 0%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/Mar27_06-52-08_c2edef9bda76/ (stored 0%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/Mar27_06-52-08_c2edef9bda76/events.out.tfevents.1743062784.c2edef9bda76.6654.1 (deflated 22%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/Mar27_06-52-08_c2edef9bda76/events.out.tfevents.1743058341.c2edef9bda76.6654.0 (deflated 60%)\n",
            "  adding: content/finetuned_qwen_ballerina/README.md (deflated 66%)\n",
            "  adding: content/finetuned_qwen_ballerina/tokenizer.json (deflated 81%)\n",
            "  adding: content/finetuned_qwen_ballerina/adapter_config.json (deflated 55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model_path = \"./finetuned_qwen_ballerina\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Function to generate Ballerina code completions\n",
        "def generate_code_completion(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
        "    # Prepare the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return the completion\n",
        "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "# Example usage\n",
        "prompt = \"public function calucalateFactorial(int number) returns int|error {\\n\"\n",
        "completion = generate_code_completion(prompt)\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Jl6MAftTeB",
        "outputId": "d5170052-e309-4003-c235-cf5e8912f758"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "public function calucalateFactorial(int number) returns int|error {\n",
            "    if number < 0 {\n",
            "        throw error(\"Factorial cannot be negative\");\n",
            "    }\n",
            "    if number == 0 {\n",
            "        return 1;\n",
            "    }\n",
            "    if number == 1 {\n",
            "        return 1;\n",
            "    }\n",
            "    return number * calucalateFactorial(number - 1);\n",
            "}\n",
            "\n",
            "// Calculate the number of combinations between n items and k items\n",
            "// using the factorial function\n",
            "function combinations(n, k) returns int|error {\n",
            "    if n < 0 || k < 0 {\n",
            "        throw error(\"n and k must be non-negative integers\");\n",
            "    }\n",
            "    if k > n {\n",
            "        return 0;\n",
            "    }\n",
            "    return calucalateFactorial(n) / (calucalateFactorial(k) * calucalateFactorial(n - k));\n",
            "}\n",
            "\n",
            "// Calculate the number of permutations between n items and k items\n",
            "// using the factorial function\n",
            "function permutations(n, k) returns int|error {\n",
            "    if n < \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with Base model"
      ],
      "metadata": {
        "id": "glGSkou4w1H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Function to generate code using any model\n",
        "def generate_with_model(model_name, prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
        "    # Load model and tokenizer\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Prepare the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return the completion\n",
        "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"public function calucalateFactorial(int number) returns int|error {\\n\"\n",
        "\n",
        "# Compare base model vs fine-tuned model\n",
        "print(\"===== BASE MODEL OUTPUT =====\")\n",
        "# Replace \"Qwen/Qwen-7B\" with the actual base model you used for fine-tuning\n",
        "base_output = generate_with_model(\"Qwen/Qwen2.5-Coder-0.5B\", prompt)\n",
        "print(base_output)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL OUTPUT =====\")\n",
        "finetuned_output = generate_with_model(\"./finetuned_qwen_ballerina\", prompt)\n",
        "print(finetuned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnSvKcWVvobM",
        "outputId": "8ad9d5fd-5108-45eb-ca19-2bd5b18c6ae0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== BASE MODEL OUTPUT =====\n",
            "Loading model: Qwen/Qwen2.5-Coder-0.5B\n",
            "public function calucalateFactorial(int number) returns int|error {\n",
            "    if (number < 0) {\n",
            "        return error(\"Factorial is not defined for negative numbers\");\n",
            "    }\n",
            "    else if (number == 0) {\n",
            "        return 1;\n",
            "    }\n",
            "    else {\n",
            "        int result = 1;\n",
            "        for (int i = 1; i <= number; i++) {\n",
            "            result *= i;\n",
            "        }\n",
            "        return result;\n",
            "    }\n",
            "}\n",
            "\n",
            "===== FINE-TUNED MODEL OUTPUT =====\n",
            "Loading model: ./finetuned_qwen_ballerina\n",
            "public function calucalateFactorial(int number) returns int|error {\n",
            "    if (number < 0) {\n",
            "        throw error(\"Factorial can not be negative\");\n",
            "    }\n",
            "    if (number <= 1) {\n",
            "        return 1;\n",
            "    }\n",
            "    if (number == 2) {\n",
            "        return 2;\n",
            "    }\n",
            "    return number * calucalateFactorial(number - 1);\n",
            "}\n",
            "/**\n",
            " * 生成一个随机的数字\n",
            " * @param min 最小值\n",
            " * @param max 最大值\n",
            " * @return 生成的数字\n",
            " */\n",
            "function randomInt(min: int, max: int) returns int {\n",
            "    if (min > max) {\n",
            "        throw error(\"min must be smaller than max\");\n",
            "    }\n",
            "    return random(min, max);\n",
            "}\n",
            "/**\n",
            " * 生成一个随机的字符串\n",
            " * @param length 长度\n",
            " * @param chars 字符串\n",
            " * @return 生成的字符串\n",
            " */\n",
            "function randomString(length: int, chars: string) returns string {\n",
            "    let result =\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"import ballerina/http;\n",
        "service / on new http:Listener(9090) {\\n\"\"\"\n",
        "\n",
        "# Compare base model vs fine-tuned model\n",
        "print(\"===== BASE MODEL OUTPUT =====\")\n",
        "# Replace \"Qwen/Qwen-7B\" with the actual base model you used for fine-tuning\n",
        "base_output = generate_with_model(\"Qwen/Qwen2.5-Coder-0.5B\", prompt)\n",
        "print(base_output)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL OUTPUT =====\")\n",
        "finetuned_output = generate_with_model(\"./finetuned_qwen_ballerina\", prompt)\n",
        "print(finetuned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffNNNUWEw9c_",
        "outputId": "20450820-7395-4da0-ceaa-20d9566172b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== BASE MODEL OUTPUT =====\n",
            "Loading model: Qwen/Qwen2.5-Coder-0.5B\n",
            "import ballerina/http;\n",
            "service / on new http:Listener(9090) {\n",
            "    @http:GET\n",
            "    public static string get() {\n",
            "        return \"Hello World\";\n",
            "    }\n",
            "}\n",
            "\n",
            "===== FINE-TUNED MODEL OUTPUT =====\n",
            "Loading model: ./finetuned_qwen_ballerina\n",
            "import ballerina/http;\n",
            "service / on new http:Listener(9090) {\n",
            "    /**\n",
            "     * The `handleRequest` method is the entry point for the HTTP service. \n",
            "     * \n",
            "     * @param request - The HTTP request is received as a JSON object.\n",
            "     * @param response - The HTTP response is sent back to the client as a JSON object.\n",
            "     * \n",
            "     * @return The `return` statement is used to indicate that the function is done and should not return any value.\n",
            "     */\n",
            "    http:Response handleRequest(http:Request request, http:Response response) {\n",
            "        if (request.method == \"POST\") {\n",
            "            // The request body is the payload of the POST request.\n",
            "            string payload = (string) request.body;\n",
            "            // Extract the JSON payload from the request body.\n",
            "            json:Value jsonPayload = json:parse(payload);\n",
            "            // Convert the JSON payload to a string.\n",
            "            string jsonString = jsonPayload.toString();\n",
            "            // Set the response body to the JSON payload.\n",
            "            response.body = jsonString;\n",
            "            // Set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"import ballerina/http;\n",
        "\n",
        "# A client class for interacting with a chat service.\n",
        "public isolated client class ChatClient {\n",
        "    private final http:Client httpClient;\n",
        "\n",
        "    # Initializes the `ChatClient` with the provided service URL and configuration.\n",
        "    #\n",
        "    # + serviceUrl - The base URL of the chat service.\n",
        "    # + clientConfig - Configuration options for the chat client.\n",
        "    # + return - An `error` if the client initialization fails otherwise nil.\n",
        "    public function init(string serviceUrl, *ChatClientConfiguration clientConfig) returns error? {\"\"\"\n",
        "\n",
        "# Compare base model vs fine-tuned model\n",
        "print(\"===== BASE MODEL OUTPUT =====\")\n",
        "# Replace \"Qwen/Qwen-7B\" with the actual base model you used for fine-tuning\n",
        "base_output = generate_with_model(\"Qwen/Qwen2.5-Coder-0.5B\", prompt)\n",
        "print(base_output)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL OUTPUT =====\")\n",
        "finetuned_output = generate_with_model(\"./finetuned_qwen_ballerina\", prompt)\n",
        "print(finetuned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8o3HeDRyPI0",
        "outputId": "53dffcbb-2137-4e86-869e-126f3dc08985"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== BASE MODEL OUTPUT =====\n",
            "Loading model: Qwen/Qwen2.5-Coder-0.5B\n",
            "import ballerina/http;\n",
            "\n",
            "# A client class for interacting with a chat service.\n",
            "public isolated client class ChatClient {\n",
            "    private final http:Client httpClient;\n",
            "\n",
            "    # Initializes the `ChatClient` with the provided service URL and configuration.\n",
            "    #\n",
            "    # + serviceUrl - The base URL of the chat service.\n",
            "    # + clientConfig - Configuration options for the chat client.\n",
            "    # + return - An `error` if the client initialization fails otherwise nil.\n",
            "    public function init(string serviceUrl, *ChatClientConfiguration clientConfig) returns error? {\n",
            "        if (serviceUrl is not string) {\n",
            "            return error(\"Service URL must be a string\");\n",
            "        }\n",
            "\n",
            "        if (clientConfig is not ChatClientConfiguration) {\n",
            "            return error(\"Client configuration must be an instance of `ChatClientConfiguration`\");\n",
            "        }\n",
            "\n",
            "        httpClient = new(http.Client { serviceUrl: serviceUrl });\n",
            "        return nil;\n",
            "    }\n",
            "\n",
            "    # Sends a message to the chat service.\n",
            "    #\n",
            "    # + message - The message to send to the chat service.\n",
            "    # + return - An `error` if the message sending fails otherwise nil.\n",
            "    public function send(string message) returns error? {\n",
            "        if (message is not string) {\n",
            "            return error(\"Message must be a string\");\n",
            "        }\n",
            "\n",
            "        return httpClient.post(\"/chat\", message);\n",
            "    }\n",
            "\n",
            "    # Retrieves messages from the chat service.\n",
            "    #\n",
            "    # + return - A `Response` containing the messages received from the chat service.\n",
            "    public function getMessages\n",
            "\n",
            "===== FINE-TUNED MODEL OUTPUT =====\n",
            "Loading model: ./finetuned_qwen_ballerina\n",
            "import ballerina/http;\n",
            "\n",
            "# A client class for interacting with a chat service.\n",
            "public isolated client class ChatClient {\n",
            "    private final http:Client httpClient;\n",
            "\n",
            "    # Initializes the `ChatClient` with the provided service URL and configuration.\n",
            "    #\n",
            "    # + serviceUrl - The base URL of the chat service.\n",
            "    # + clientConfig - Configuration options for the chat client.\n",
            "    # + return - An `error` if the client initialization fails otherwise nil.\n",
            "    public function init(string serviceUrl, *ChatClientConfiguration clientConfig) returns error? {\n",
            "        this.httpClient = new Client(serviceUrl, clientConfig);\n",
            "        return null;\n",
            "    }\n",
            "\n",
            "    # Sends a request to the chat service and returns the response.\n",
            "    #\n",
            "    # + query - The query to be sent to the chat service.\n",
            "    # + return - The response received from the chat service, or an error if the request fails.\n",
            "    public async function send(string query) returns error? {\n",
            "        return await this.httpClient.post(\"/chat\", headers: [\"Content-Type: application/json\"], json: query);\n",
            "    }\n",
            "}\n",
            "# A configuration object for the `ChatClient`.\n",
            "public interface ChatClientConfiguration {\n",
            "    # The base URL of the chat service.\n",
            "    # + type - The type of the URL, which is HTTP.\n",
            "    # + value - The value of the URL.\n",
            "    # + return - An `error` if the base URL is incorrect.\n",
            "    public function url(string type, string value) returns error? {\n",
            "        if (type != \"http\") {\n",
            "           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to Ollama"
      ],
      "metadata": {
        "id": "gfB4roP58S7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers peft accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load the fine-tuned model and tokenizer from the saved directory\n",
        "model_path = \"./finetuned_qwen_ballerina\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Merge LoRA weights into the base model\n",
        "# Note: If your model is already a PeftModel with LoRA, this step merges it\n",
        "if hasattr(model, \"merge_and_unload\"):\n",
        "    print(\"Merging LoRA weights...\")\n",
        "    model = model.merge_and_unload()  # Merges LoRA weights into the base model and unloads adapters\n",
        "else:\n",
        "    print(\"Model does not have LoRA weights to merge, proceeding with base model.\")\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Save the merged model and tokenizer to a new directory for Ollama\n",
        "ollama_model_path = \"./ollama_finetuned_qwen_ballerina\"\n",
        "model.save_pretrained(ollama_model_path)\n",
        "tokenizer.save_pretrained(ollama_model_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {ollama_model_path}. Ready for Ollama export.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "lneEPPDD5xGG",
        "outputId": "8534af31-7c51-4fb7-c872-153a7fdb93d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized model in ./finetuned_qwen_ballerina. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe3bcd249eb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the fine-tuned model and tokenizer from the saved directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./finetuned_qwen_ballerina\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;34mf\"Should have a `model_type` key in its {CONFIG_NAME}, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized model in ./finetuned_qwen_ballerina. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instruct..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbJB12zj8X5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}