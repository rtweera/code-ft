{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtweera/code-ft/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training a Coding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers peft accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import os\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuring Tensor Processing Unit (TPU) for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_98wkmGuL1CJ",
        "outputId": "32bcac18-f61c-4243-a5a7-88075ddfafd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPU not found. `is_xla` set to `False`\n",
            "Using CPU with fp32 (Full Precision) precision\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    is_xla = True\n",
        "except:\n",
        "    print(\"TPU not found. `is_xla` set to `False`\")\n",
        "    is_xla = False\n",
        "\n",
        "# Initialize accelerator with bf16 (brain float 16 - special FP16 dtype used by Google TPU) if using TPU \n",
        "if is_xla:\n",
        "    os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
        "    from accelerate import Accelerator  # Import Accelerator ONLY after setting environment variable\n",
        "    accelerator = Accelerator(mixed_precision=\"bf16\")\n",
        "    precision = \"bf16 (Brain Float 16)\"\n",
        "    print(f\"Using TPU with {precision} precision\")\n",
        "else:\n",
        "    from accelerate import Accelerator\n",
        "    # Set default to fp16 for GPU or no mixed precision for CPU\n",
        "    if torch.cuda.is_available():\n",
        "        accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "        precision = \"fp16 (Half Precision)\"\n",
        "    else:\n",
        "        accelerator = Accelerator(mixed_precision=\"no\")\n",
        "        precision = \"fp32 (Full Precision)\"\n",
        "    \n",
        "    print(f\"Using {'GPU' if torch.cuda.is_available() else 'CPU'} with {precision} precision\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BF16\n",
        "\n",
        "* BF16 is a numerical format that is similar to FP16 but has a wider dynamic range, making it more suitable for training large models on TPUs. \n",
        "* It is designed to provide better performance and stability during training, especially for large models.\n",
        "* BF16 is not natively supported on GPUs, so if you are using a GPU, you can use FP16 instead.\n",
        "* BF16 uses 8 bits for the exponent and 7 bits for the mantissa, while FP16 uses 5 bits for the exponent and 10 bits for the mantissa.\n",
        "* This means that BF16 can represent a wider range of values than FP16, which can help prevent underflow and overflow during training.\n",
        "\n",
        "#### Mixed precision\n",
        "* Mixed precision training is a technique that uses both 16-bit and 32-bit floating-point numbers to speed up training and reduce memory usage.\n",
        "* In mixed precision training, the model's weights and gradients are stored in 16-bit format, while the optimizer state and loss are stored in 32-bit format.\n",
        "* This allows the model to take advantage of the speed and memory benefits of 16-bit training while still maintaining the numerical stability of 32-bit training.\n",
        "* Mixed precision training is supported on both TPUs and GPUs, and it can significantly speed up training times while reducing memory usage.\n",
        "\n",
        "#### FP16 vs FP32\n",
        "* GPUs can use FP16 (16-bit floating-point) or FP32 (32-bit floating-point) for training.\n",
        "* CPUs typically use FP32 for training, as they do not have native support for FP16, and running FP16 on CPUs can lead to additional overhead and slower performance as CPUs need to convert FP16 to FP32 to process.\n",
        "\n",
        "#### Gradient checkpointing\n",
        "* Gradient checkpointing is a technique that reduces memory usage during training by storing only a subset of the intermediate activations and recomputing the rest during the backward pass.\n",
        "* This allows for training larger models on limited hardware resources, such as TPUs or GPUs with limited memory.\n",
        "* Gradient checkpointing works by dividing the model into segments and storing only the activations of the segments that are needed for the backward pass.\n",
        "* The rest of the activations are recomputed during the backward pass, which reduces memory usage at the cost of increased computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base model & Tokenizer loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "k_70N7POGvvE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Model: Qwen/Qwen2.5-Coder-0.5B ===\n",
            "Model type: Qwen2ForCausalLM\n",
            "Model size: 494.03M parameters\n",
            "Model architecture: qwen2\n",
            "Number of layers: 24\n",
            "Hidden size: 896\n",
            "Attention heads: 14\n",
            "Vocab size: 151936\n",
            "\n",
            "=== Tokenizer ===\n",
            "Tokenizer type: Qwen2TokenizerFast\n",
            "Vocabulary size: 151665\n",
            "Model max length: 32768\n",
            "\n",
            "=== Special Tokens ===\n",
            "eos_token: '<|endoftext|>' (ID: 151643)\n",
            "pad_token: '<|endoftext|>' (ID: 151643)\n",
            "additional_special_tokens: '<|im_start|>' (ID: 151644)\n",
            "additional_special_tokens: '<|im_end|>' (ID: 151645)\n",
            "additional_special_tokens: '<|object_ref_start|>' (ID: 151646)\n",
            "additional_special_tokens: '<|object_ref_end|>' (ID: 151647)\n",
            "additional_special_tokens: '<|box_start|>' (ID: 151648)\n",
            "additional_special_tokens: '<|box_end|>' (ID: 151649)\n",
            "additional_special_tokens: '<|quad_start|>' (ID: 151650)\n",
            "additional_special_tokens: '<|quad_end|>' (ID: 151651)\n",
            "additional_special_tokens: '<|vision_start|>' (ID: 151652)\n",
            "additional_special_tokens: '<|vision_end|>' (ID: 151653)\n",
            "additional_special_tokens: '<|vision_pad|>' (ID: 151654)\n",
            "additional_special_tokens: '<|image_pad|>' (ID: 151655)\n",
            "additional_special_tokens: '<|video_pad|>' (ID: 151656)\n",
            "\n",
            "Is Fast Tokenizer: True\n",
            "\n",
            "=== Example Encoding ===\n",
            "Raw encoded: {'input_ids': tensor([[  750, 11047, 18588,   530,  1445,  1648]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
            "Text: 'def calculate_factorial(n):'\n",
            "Token IDs: [750, 11047, 18588, 530, 1445, 1648]\n",
            "Decoded: 'def calculate_factorial(n):'\n",
            "Number of tokens: 6\n"
          ]
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B\"  # Specify exact variant if needed (e.g., 7B, 1.5B)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Display model details\n",
        "print(f\"=== Model: {model_name} ===\")\n",
        "print(f\"Model type: {model.__class__.__name__}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
        "print(f\"Model architecture: {model.config.model_type}\")\n",
        "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"Attention heads: {model.config.num_attention_heads}\")\n",
        "print(f\"Vocab size: {model.config.vocab_size}\")\n",
        "\n",
        "# Display tokenizer details\n",
        "print(f\"\\n=== Tokenizer ===\")\n",
        "print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
        "\n",
        "# Display special tokens\n",
        "print(f\"\\n=== Special Tokens ===\")\n",
        "for key, value in tokenizer.special_tokens_map.items():\n",
        "    if isinstance(value, str):\n",
        "        print(f\"{key}: '{value}' (ID: {tokenizer.convert_tokens_to_ids(value)})\")\n",
        "    elif isinstance(value, list):\n",
        "        for v in value:\n",
        "            print(f\"{key}: '{v}' (ID: {tokenizer.convert_tokens_to_ids(v)})\")\n",
        "        # print(f\"{key}: '{value}' (ID: {tokenizer.convert_tokens_to_ids(value)})\")\n",
        "\n",
        "# Optional: Check and display if tokenizer is fast (fast ones are written in Rust; slow ones are in Python)\n",
        "if hasattr(tokenizer, \"is_fast\"):\n",
        "    print(f\"\\nIs Fast Tokenizer: {tokenizer.is_fast}\")\n",
        "\n",
        "# Optional: Display example encoding\n",
        "example_text = \"def calculate_factorial(n):\"\n",
        "encoding = tokenizer(example_text, return_tensors=\"pt\")\n",
        "decoded = tokenizer.decode(encoding.input_ids[0])\n",
        "print(f\"\\n=== Example Encoding ===\")\n",
        "print(f\"Raw encoded: {encoding}\")\n",
        "print(f\"Text: '{example_text}'\")\n",
        "print(f\"Token IDs: {encoding.input_ids[0].tolist()}\")\n",
        "print(f\"Decoded: '{decoded}'\")\n",
        "print(f\"Number of tokens: {len(encoding.input_ids[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tokenizer examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Attention Mask Examples ===\n",
            "\n",
            "1. Single sentence (all tokens attended to):\n",
            "Text: 'def add(a, b):'\n",
            "Input IDs: [750, 912, 2877, 11, 293, 1648]\n",
            "Attention mask: [1, 1, 1, 1, 1, 1]\n",
            "All 1's in attention mask = attend to all tokens\n",
            "\n",
            "2. Padded batch (shorter sequence has padding tokens):\n",
            "Texts: ['def factorial(n):', 'def sum_array(arr):']\n",
            "\n",
            "Sequence 1: 'def factorial(n):'\n",
            "Input IDs: [750, 52962, 1445, 1648, 151643]\n",
            "Attention mask: [1, 1, 1, 1, 0]\n",
            "0's in mask = ignore these tokens (padding)\n",
            "\n",
            "Sequence 2: 'def sum_array(arr):'\n",
            "Input IDs: [750, 2629, 3858, 10939, 1648]\n",
            "Attention mask: [1, 1, 1, 1, 1]\n",
            "0's in mask = ignore these tokens (padding)\n",
            "\n",
            "3. Code with comment:\n",
            "Text: 'def multiply(x, y):  # Multiplies two numbers'\n",
            "Input IDs: [750, 30270, 2075, 11, 379, 1648, 220, 671, 17439, 7202, 1378, 5109]\n",
            "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "4. Visual representation of attention masks:\n",
            "'def factorial(n):': ■■■■□\n",
            "'def sum_array(arr):': ■■■■■\n",
            "\n",
            "5. Token-by-token breakdown with attention:\n",
            "\n",
            "Sequence 1: 'def factorial(n):'\n",
            "Token                | Token ID | Attended?  | Token Text\n",
            "----------------------------------------------------------------------\n",
            "def                  | 750      | Yes        | 'def'\n",
            "Ġfactorial           | 52962    | Yes        | '·factorial'\n",
            "(n                   | 1445     | Yes        | '(n'\n",
            "):                   | 1648     | Yes        | '):'\n",
            "<|endoftext|>        | 151643   | No (padding) | '<|endoftext|>'\n",
            "\n",
            "Sequence 2: 'def sum_array(arr):'\n",
            "Token                | Token ID | Attended?  | Token Text\n",
            "----------------------------------------------------------------------\n",
            "def                  | 750      | Yes        | 'def'\n",
            "Ġsum                 | 2629     | Yes        | '·sum'\n",
            "_array               | 3858     | Yes        | '_array'\n",
            "(arr                 | 10939    | Yes        | '(arr'\n",
            "):                   | 1648     | Yes        | '):'\n"
          ]
        }
      ],
      "source": [
        "# Examples of different attention masks\n",
        "print(\"\\n=== Attention Mask Examples ===\")\n",
        "\n",
        "# Example 1: Single sentence - all tokens are attended to\n",
        "single_text = \"def add(a, b):\"\n",
        "single_encoding = tokenizer(single_text, return_tensors=\"pt\")\n",
        "print(\"\\n1. Single sentence (all tokens attended to):\")\n",
        "print(f\"Text: '{single_text}'\")\n",
        "print(f\"Input IDs: {single_encoding.input_ids[0].tolist()}\")\n",
        "print(f\"Attention mask: {single_encoding.attention_mask[0].tolist()}\")\n",
        "print(f\"All 1's in attention mask = attend to all tokens\")\n",
        "\n",
        "# Example 2: Padded sequence - some tokens should be ignored\n",
        "texts = [\"def factorial(n):\", \"def sum_array(arr):\"]\n",
        "padded_encoding = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "print(\"\\n2. Padded batch (shorter sequence has padding tokens):\")\n",
        "print(f\"Texts: {texts}\")\n",
        "for i, (text, ids, mask) in enumerate(zip(texts, padded_encoding.input_ids, padded_encoding.attention_mask)):\n",
        "    decoded_ids = tokenizer.decode(ids)\n",
        "    print(f\"\\nSequence {i+1}: '{text}'\")\n",
        "    print(f\"Input IDs: {ids.tolist()}\")  \n",
        "    print(f\"Attention mask: {mask.tolist()}\")\n",
        "    print(f\"0's in mask = ignore these tokens (padding)\")\n",
        "\n",
        "# Example 3: Code with comments that might be treated differently\n",
        "code_with_comment = \"def multiply(x, y):  # Multiplies two numbers\"\n",
        "comment_encoding = tokenizer(code_with_comment, return_tensors=\"pt\")\n",
        "print(\"\\n3. Code with comment:\")\n",
        "print(f\"Text: '{code_with_comment}'\")\n",
        "print(f\"Input IDs: {comment_encoding.input_ids[0].tolist()}\")\n",
        "print(f\"Attention mask: {comment_encoding.attention_mask[0].tolist()}\")\n",
        "\n",
        "# Example 4: Visualize the attention mask\n",
        "print(\"\\n4. Visual representation of attention masks:\")\n",
        "for i, (text, mask) in enumerate(zip(texts, padded_encoding.attention_mask)):\n",
        "    att_vis = ''.join(['■' if m == 1 else '□' for m in mask])\n",
        "    print(f\"'{text}': {att_vis}\")\n",
        "    \n",
        "# Breakdown of tokens for the second example\n",
        "print(\"\\n5. Token-by-token breakdown with attention:\")\n",
        "for i, (text, ids, mask) in enumerate(zip(texts, padded_encoding.input_ids, padded_encoding.attention_mask)):\n",
        "    print(f\"\\nSequence {i+1}: '{text}'\")\n",
        "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "    print(f\"{'Token':<20} | {'Token ID':<8} | {'Attended?':<10} | {'Token Text'}\")\n",
        "    print(\"-\" * 70)\n",
        "    for token, token_id, attention in zip(tokens, ids.tolist(), mask.tolist()):\n",
        "        attended = \"Yes\" if attention == 1 else \"No (padding)\"\n",
        "        token_text = tokenizer.decode([token_id]).replace(\" \", \"·\")\n",
        "        print(f\"{token:<20} | {token_id:<8} | {attended:<10} | '{token_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Padding or truncating is done when we are dealing with batches of text sequences of different lengths.\n",
        "* Padding: Adding special tokens to the end of a sequence to make it the same length as the longest sequence in the batch.\n",
        "* Truncating: Removing tokens from the end of a sequence to make it shorter than the maximum length.\n",
        "* Padding and truncating are important for batch processing, as they ensure that all sequences in a batch have the same length, which is required for efficient computation on TPUs or GPUs.\n",
        "* If not, tokenizer will throw an error when trying to process a batch of sequences with different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WKaEOsbnGvvG"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(texts, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m, padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Tokenize datasets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      7\u001b[39m     train_data = f.readlines()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mval.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ravindu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train.txt'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize datasets\n",
        "with open(\"train.txt\", \"r\") as f:\n",
        "    train_data = f.readlines()\n",
        "with open(\"val.txt\", \"r\") as f:\n",
        "    val_data = f.readlines()\n",
        "\n",
        "train_tokenized = tokenize_function(train_data)\n",
        "val_tokenized = tokenize_function(val_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeaZl9pdGvvI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert to a format suitable for Trainer\n",
        "class SimpleDataset:\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = tokenized_data[\"input_ids\"]\n",
        "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.input_ids[idx], \"attention_mask\": self.attention_mask[idx]}\n",
        "\n",
        "train_dataset = SimpleDataset(train_tokenized)\n",
        "val_dataset = SimpleDataset(val_tokenized)\n",
        "\n",
        "# Data collator for CLM (shifts inputs to create targets)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ercAJ3tHGvvJ",
        "outputId": "db4e8551-ce8d-4887-d256-94de04a8ebc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,              # Rank of the adaptation matrices # TODO: checck\n",
        "    lora_alpha=32,    # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers to adapt\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_qwen_ballerina\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=is_xla,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "if is_xla:\n",
        "  trainer = accelerator.prepare(trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qlff1TCUGvvK",
        "outputId": "0fc495d5-3e81-4aa2-8cee-dd2da7f50f63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrtweera\u001b[0m (\u001b[33mrtw-rtweera\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_065555-bwdu714n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rtw-rtweera/huggingface/runs/bwdu714n' target=\"_blank\">./finetuned_qwen_ballerina</a></strong> to <a href='https://wandb.ai/rtw-rtweera/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rtw-rtweera/huggingface' target=\"_blank\">https://wandb.ai/rtw-rtweera/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rtw-rtweera/huggingface/runs/bwdu714n' target=\"_blank\">https://wandb.ai/rtw-rtweera/huggingface/runs/bwdu714n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2215' max='6642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2215/6642 54:10 < 1:48:21, 0.68 it/s, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.396600</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyError",
          "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_determine_best_metric\u001b[0;34m(self, metrics, trial)\u001b[0m\n\u001b[1;32m   3150\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3151\u001b[0;31m                 \u001b[0mmetric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3152\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eval_loss'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e5e6b3b9574e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSaveStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_determine_best_metric\u001b[0;34m(self, metrics, trial)\u001b[0m\n\u001b[1;32m   3151\u001b[0m                 \u001b[0mmetric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3152\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3153\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   3154\u001b[0m                     \u001b[0;34mf\"The `metric_for_best_model` training argument is set to '{metric_to_check}', which is not found in the evaluation metrics. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m                     \u001b[0;34mf\"The available evaluation metrics are: {list(metrics.keys())}. Consider changing the `metric_for_best_model` via the TrainingArguments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "2mmq8ldLnWuB",
        "outputId": "42aaab7b-f43d-457e-dcd9-365f1f1ef742"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='567' max='567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [567/567 06:19]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_model_preparation_time': 0.0465, 'eval_runtime': 379.3769, 'eval_samples_per_second': 5.973, 'eval_steps_per_second': 1.495}\n"
          ]
        }
      ],
      "source": [
        "# Manual evaluation\n",
        "from transformers import Trainer\n",
        "temp_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "metrics = temp_trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2n5rMKIGvvL",
        "outputId": "382a09d4-7277-405d-fd8c-f0a76ce455c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./finetuned_qwen_ballerina/tokenizer_config.json',\n",
              " './finetuned_qwen_ballerina/special_tokens_map.json',\n",
              " './finetuned_qwen_ballerina/vocab.json',\n",
              " './finetuned_qwen_ballerina/merges.txt',\n",
              " './finetuned_qwen_ballerina/added_tokens.json',\n",
              " './finetuned_qwen_ballerina/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./finetuned_qwen_ballerina\")\n",
        "tokenizer.save_pretrained(\"./finetuned_qwen_ballerina\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqMQOPCI9hs4"
      },
      "source": [
        "### Zip and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaMUqW03s__-",
        "outputId": "58864d4e-2f37-4929-9caa-f96dd4f63ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/finetuned_qwen_ballerina/ (stored 0%)\n",
            "  adding: content/finetuned_qwen_ballerina/merges.txt (deflated 57%)\n",
            "  adding: content/finetuned_qwen_ballerina/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/finetuned_qwen_ballerina/special_tokens_map.json (deflated 69%)\n",
            "  adding: content/finetuned_qwen_ballerina/vocab.json (deflated 61%)\n",
            "  adding: content/finetuned_qwen_ballerina/tokenizer_config.json (deflated 83%)\n",
            "  adding: content/finetuned_qwen_ballerina/added_tokens.json (deflated 67%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/ (stored 0%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/Mar27_06-52-08_c2edef9bda76/ (stored 0%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/Mar27_06-52-08_c2edef9bda76/events.out.tfevents.1743062784.c2edef9bda76.6654.1 (deflated 22%)\n",
            "  adding: content/finetuned_qwen_ballerina/runs/Mar27_06-52-08_c2edef9bda76/events.out.tfevents.1743058341.c2edef9bda76.6654.0 (deflated 60%)\n",
            "  adding: content/finetuned_qwen_ballerina/README.md (deflated 66%)\n",
            "  adding: content/finetuned_qwen_ballerina/tokenizer.json (deflated 81%)\n",
            "  adding: content/finetuned_qwen_ballerina/adapter_config.json (deflated 55%)\n"
          ]
        }
      ],
      "source": [
        "# prompt: zip this file for downloading \"./finetuned_qwen_ballerina\"\n",
        "\n",
        "!zip -r /content/finetuned_qwen_ballerina.zip /content/finetuned_qwen_ballerina\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/finetuned_qwen_ballerina.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Jl6MAftTeB",
        "outputId": "d5170052-e309-4003-c235-cf5e8912f758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "public function calucalateFactorial(int number) returns int|error {\n",
            "    if number < 0 {\n",
            "        throw error(\"Factorial cannot be negative\");\n",
            "    }\n",
            "    if number == 0 {\n",
            "        return 1;\n",
            "    }\n",
            "    if number == 1 {\n",
            "        return 1;\n",
            "    }\n",
            "    return number * calucalateFactorial(number - 1);\n",
            "}\n",
            "\n",
            "// Calculate the number of combinations between n items and k items\n",
            "// using the factorial function\n",
            "function combinations(n, k) returns int|error {\n",
            "    if n < 0 || k < 0 {\n",
            "        throw error(\"n and k must be non-negative integers\");\n",
            "    }\n",
            "    if k > n {\n",
            "        return 0;\n",
            "    }\n",
            "    return calucalateFactorial(n) / (calucalateFactorial(k) * calucalateFactorial(n - k));\n",
            "}\n",
            "\n",
            "// Calculate the number of permutations between n items and k items\n",
            "// using the factorial function\n",
            "function permutations(n, k) returns int|error {\n",
            "    if n < \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model_path = \"./finetuned_qwen_ballerina\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Function to generate Ballerina code completions\n",
        "def generate_code_completion(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
        "    # Prepare the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return the completion\n",
        "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "# Example usage\n",
        "prompt = \"public function calucalateFactorial(int number) returns int|error {\\n\"\n",
        "completion = generate_code_completion(prompt)\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glGSkou4w1H-"
      },
      "source": [
        "Compare with Base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnSvKcWVvobM",
        "outputId": "8ad9d5fd-5108-45eb-ca19-2bd5b18c6ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== BASE MODEL OUTPUT =====\n",
            "Loading model: Qwen/Qwen2.5-Coder-0.5B\n",
            "public function calucalateFactorial(int number) returns int|error {\n",
            "    if (number < 0) {\n",
            "        return error(\"Factorial is not defined for negative numbers\");\n",
            "    }\n",
            "    else if (number == 0) {\n",
            "        return 1;\n",
            "    }\n",
            "    else {\n",
            "        int result = 1;\n",
            "        for (int i = 1; i <= number; i++) {\n",
            "            result *= i;\n",
            "        }\n",
            "        return result;\n",
            "    }\n",
            "}\n",
            "\n",
            "===== FINE-TUNED MODEL OUTPUT =====\n",
            "Loading model: ./finetuned_qwen_ballerina\n",
            "public function calucalateFactorial(int number) returns int|error {\n",
            "    if (number < 0) {\n",
            "        throw error(\"Factorial can not be negative\");\n",
            "    }\n",
            "    if (number <= 1) {\n",
            "        return 1;\n",
            "    }\n",
            "    if (number == 2) {\n",
            "        return 2;\n",
            "    }\n",
            "    return number * calucalateFactorial(number - 1);\n",
            "}\n",
            "/**\n",
            " * 生成一个随机的数字\n",
            " * @param min 最小值\n",
            " * @param max 最大值\n",
            " * @return 生成的数字\n",
            " */\n",
            "function randomInt(min: int, max: int) returns int {\n",
            "    if (min > max) {\n",
            "        throw error(\"min must be smaller than max\");\n",
            "    }\n",
            "    return random(min, max);\n",
            "}\n",
            "/**\n",
            " * 生成一个随机的字符串\n",
            " * @param length 长度\n",
            " * @param chars 字符串\n",
            " * @return 生成的字符串\n",
            " */\n",
            "function randomString(length: int, chars: string) returns string {\n",
            "    let result =\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Function to generate code using any model\n",
        "def generate_with_model(model_name, prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
        "    # Load model and tokenizer\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Prepare the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return the completion\n",
        "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"public function calucalateFactorial(int number) returns int|error {\\n\"\n",
        "\n",
        "# Compare base model vs fine-tuned model\n",
        "print(\"===== BASE MODEL OUTPUT =====\")\n",
        "# Replace \"Qwen/Qwen-7B\" with the actual base model you used for fine-tuning\n",
        "base_output = generate_with_model(\"Qwen/Qwen2.5-Coder-0.5B\", prompt)\n",
        "print(base_output)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL OUTPUT =====\")\n",
        "finetuned_output = generate_with_model(\"./finetuned_qwen_ballerina\", prompt)\n",
        "print(finetuned_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffNNNUWEw9c_",
        "outputId": "20450820-7395-4da0-ceaa-20d9566172b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== BASE MODEL OUTPUT =====\n",
            "Loading model: Qwen/Qwen2.5-Coder-0.5B\n",
            "import ballerina/http;\n",
            "service / on new http:Listener(9090) {\n",
            "    @http:GET\n",
            "    public static string get() {\n",
            "        return \"Hello World\";\n",
            "    }\n",
            "}\n",
            "\n",
            "===== FINE-TUNED MODEL OUTPUT =====\n",
            "Loading model: ./finetuned_qwen_ballerina\n",
            "import ballerina/http;\n",
            "service / on new http:Listener(9090) {\n",
            "    /**\n",
            "     * The `handleRequest` method is the entry point for the HTTP service. \n",
            "     * \n",
            "     * @param request - The HTTP request is received as a JSON object.\n",
            "     * @param response - The HTTP response is sent back to the client as a JSON object.\n",
            "     * \n",
            "     * @return The `return` statement is used to indicate that the function is done and should not return any value.\n",
            "     */\n",
            "    http:Response handleRequest(http:Request request, http:Response response) {\n",
            "        if (request.method == \"POST\") {\n",
            "            // The request body is the payload of the POST request.\n",
            "            string payload = (string) request.body;\n",
            "            // Extract the JSON payload from the request body.\n",
            "            json:Value jsonPayload = json:parse(payload);\n",
            "            // Convert the JSON payload to a string.\n",
            "            string jsonString = jsonPayload.toString();\n",
            "            // Set the response body to the JSON payload.\n",
            "            response.body = jsonString;\n",
            "            // Set\n"
          ]
        }
      ],
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"import ballerina/http;\n",
        "service / on new http:Listener(9090) {\\n\"\"\"\n",
        "\n",
        "# Compare base model vs fine-tuned model\n",
        "print(\"===== BASE MODEL OUTPUT =====\")\n",
        "# Replace \"Qwen/Qwen-7B\" with the actual base model you used for fine-tuning\n",
        "base_output = generate_with_model(\"Qwen/Qwen2.5-Coder-0.5B\", prompt)\n",
        "print(base_output)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL OUTPUT =====\")\n",
        "finetuned_output = generate_with_model(\"./finetuned_qwen_ballerina\", prompt)\n",
        "print(finetuned_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8o3HeDRyPI0",
        "outputId": "53dffcbb-2137-4e86-869e-126f3dc08985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== BASE MODEL OUTPUT =====\n",
            "Loading model: Qwen/Qwen2.5-Coder-0.5B\n",
            "import ballerina/http;\n",
            "\n",
            "# A client class for interacting with a chat service.\n",
            "public isolated client class ChatClient {\n",
            "    private final http:Client httpClient;\n",
            "\n",
            "    # Initializes the `ChatClient` with the provided service URL and configuration.\n",
            "    #\n",
            "    # + serviceUrl - The base URL of the chat service.\n",
            "    # + clientConfig - Configuration options for the chat client.\n",
            "    # + return - An `error` if the client initialization fails otherwise nil.\n",
            "    public function init(string serviceUrl, *ChatClientConfiguration clientConfig) returns error? {\n",
            "        if (serviceUrl is not string) {\n",
            "            return error(\"Service URL must be a string\");\n",
            "        }\n",
            "\n",
            "        if (clientConfig is not ChatClientConfiguration) {\n",
            "            return error(\"Client configuration must be an instance of `ChatClientConfiguration`\");\n",
            "        }\n",
            "\n",
            "        httpClient = new(http.Client { serviceUrl: serviceUrl });\n",
            "        return nil;\n",
            "    }\n",
            "\n",
            "    # Sends a message to the chat service.\n",
            "    #\n",
            "    # + message - The message to send to the chat service.\n",
            "    # + return - An `error` if the message sending fails otherwise nil.\n",
            "    public function send(string message) returns error? {\n",
            "        if (message is not string) {\n",
            "            return error(\"Message must be a string\");\n",
            "        }\n",
            "\n",
            "        return httpClient.post(\"/chat\", message);\n",
            "    }\n",
            "\n",
            "    # Retrieves messages from the chat service.\n",
            "    #\n",
            "    # + return - A `Response` containing the messages received from the chat service.\n",
            "    public function getMessages\n",
            "\n",
            "===== FINE-TUNED MODEL OUTPUT =====\n",
            "Loading model: ./finetuned_qwen_ballerina\n",
            "import ballerina/http;\n",
            "\n",
            "# A client class for interacting with a chat service.\n",
            "public isolated client class ChatClient {\n",
            "    private final http:Client httpClient;\n",
            "\n",
            "    # Initializes the `ChatClient` with the provided service URL and configuration.\n",
            "    #\n",
            "    # + serviceUrl - The base URL of the chat service.\n",
            "    # + clientConfig - Configuration options for the chat client.\n",
            "    # + return - An `error` if the client initialization fails otherwise nil.\n",
            "    public function init(string serviceUrl, *ChatClientConfiguration clientConfig) returns error? {\n",
            "        this.httpClient = new Client(serviceUrl, clientConfig);\n",
            "        return null;\n",
            "    }\n",
            "\n",
            "    # Sends a request to the chat service and returns the response.\n",
            "    #\n",
            "    # + query - The query to be sent to the chat service.\n",
            "    # + return - The response received from the chat service, or an error if the request fails.\n",
            "    public async function send(string query) returns error? {\n",
            "        return await this.httpClient.post(\"/chat\", headers: [\"Content-Type: application/json\"], json: query);\n",
            "    }\n",
            "}\n",
            "# A configuration object for the `ChatClient`.\n",
            "public interface ChatClientConfiguration {\n",
            "    # The base URL of the chat service.\n",
            "    # + type - The type of the URL, which is HTTP.\n",
            "    # + value - The value of the URL.\n",
            "    # + return - An `error` if the base URL is incorrect.\n",
            "    public function url(string type, string value) returns error? {\n",
            "        if (type != \"http\") {\n",
            "           \n"
          ]
        }
      ],
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"import ballerina/http;\n",
        "\n",
        "# A client class for interacting with a chat service.\n",
        "public isolated client class ChatClient {\n",
        "    private final http:Client httpClient;\n",
        "\n",
        "    # Initializes the `ChatClient` with the provided service URL and configuration.\n",
        "    #\n",
        "    # + serviceUrl - The base URL of the chat service.\n",
        "    # + clientConfig - Configuration options for the chat client.\n",
        "    # + return - An `error` if the client initialization fails otherwise nil.\n",
        "    public function init(string serviceUrl, *ChatClientConfiguration clientConfig) returns error? {\"\"\"\n",
        "\n",
        "# Compare base model vs fine-tuned model\n",
        "print(\"===== BASE MODEL OUTPUT =====\")\n",
        "# Replace \"Qwen/Qwen-7B\" with the actual base model you used for fine-tuning\n",
        "base_output = generate_with_model(\"Qwen/Qwen2.5-Coder-0.5B\", prompt)\n",
        "print(base_output)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL OUTPUT =====\")\n",
        "finetuned_output = generate_with_model(\"./finetuned_qwen_ballerina\", prompt)\n",
        "print(finetuned_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfB4roP58S7X"
      },
      "source": [
        "### Export to Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "lneEPPDD5xGG",
        "outputId": "8534af31-7c51-4fb7-c872-153a7fdb93d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unrecognized model in ./finetuned_qwen_ballerina. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe3bcd249eb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the fine-tuned model and tokenizer from the saved directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./finetuned_qwen_ballerina\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;34mf\"Should have a `model_type` key in its {CONFIG_NAME}, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized model in ./finetuned_qwen_ballerina. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instruct..."
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers peft accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load the fine-tuned model and tokenizer from the saved directory\n",
        "model_path = \"./finetuned_qwen_ballerina\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Merge LoRA weights into the base model\n",
        "# Note: If your model is already a PeftModel with LoRA, this step merges it\n",
        "if hasattr(model, \"merge_and_unload\"):\n",
        "    print(\"Merging LoRA weights...\")\n",
        "    model = model.merge_and_unload()  # Merges LoRA weights into the base model and unloads adapters\n",
        "else:\n",
        "    print(\"Model does not have LoRA weights to merge, proceeding with base model.\")\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Save the merged model and tokenizer to a new directory for Ollama\n",
        "ollama_model_path = \"./ollama_finetuned_qwen_ballerina\"\n",
        "model.save_pretrained(ollama_model_path)\n",
        "tokenizer.save_pretrained(ollama_model_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {ollama_model_path}. Ready for Ollama export.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbJB12zj8X5u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
