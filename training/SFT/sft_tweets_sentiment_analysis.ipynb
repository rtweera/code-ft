{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets pandas transformers huggingface_hub ipywidgets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  label  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going      1   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2  088c60f138                          my boss is bullying me...      0   \n",
       "3  9642c003ef                     what interview! leave me alone      0   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...      1   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...      2   \n",
       "7  50e14c0bb8                                         Soooo high      1   \n",
       "8  e050245fbd                                        Both of you      1   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....      2   \n",
       "\n",
       "  label_text  \n",
       "0    neutral  \n",
       "1   negative  \n",
       "2   negative  \n",
       "3   negative  \n",
       "4   negative  \n",
       "5    neutral  \n",
       "6   positive  \n",
       "7    neutral  \n",
       "8    neutral  \n",
       "9   positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer is a special class in the Huggingface Transformers library. It helps you choose the right tokenizer for your model without knowing the details.\n",
    "\n",
    "Think of it as a smart assistant that knows which tool to use for the job.\n",
    "\n",
    "The AutoTokenizer is easy to use. You donâ€™t have to remember which tokenizer goes with which model. It ensures you use the correct tokenizer for the model, reducing errors and improving consistency.\n",
    "\n",
    "Autotokenizer is flexible. It works with many different models, allowing you to switch models without changing much code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to Hugging Face Hub\n",
      "Loaded Tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the token from the environment variables\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "try:\n",
    "    login(token=token)\n",
    "    print(\"Logged in to Hugging Face Hub\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during login: {e}\")\n",
    "    print(\"Please ensure you have a valid Hugging Face token in your .env file.\")\n",
    "    exit(1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "print(\"Loaded Tokenizer\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-3.2-1B\", num_labels=3)    \n",
    "print(\"Loaded Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and Encode Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pad token. By default, LLaMA models do not have a pad token, so we need to set it manually.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # padding set make the batches equal to the value set in max_length \n",
    "    # truncation set to True to truncate the text if it exceeds max_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# print(tokenized_datasets)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training started!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1000 : < :, Epoch 0.00/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Custom callback to visualize training progress\n",
    "class ProgressVisualizationCallback(TrainerCallback):  # Inherit from TrainerCallback\n",
    "    def __init__(self):\n",
    "        self.training_logs = []\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        # Initialize at the beginning of training\n",
    "        self.training_logs = []\n",
    "        print(\"Training started!\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            self.training_logs.append(logs)\n",
    "            # Plot every 10 logs to avoid slowing down training\n",
    "            if len(self.training_logs) % 10 == 0:\n",
    "                self.visualize_progress(state)\n",
    "    \n",
    "    def visualize_progress(self, state):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Extract metrics\n",
    "        steps = [log.get('step', i) for i, log in enumerate(self.training_logs) if 'loss' in log]\n",
    "        loss = [log['loss'] for log in self.training_logs if 'loss' in log]\n",
    "        lr = [log['learning_rate'] for log in self.training_logs if 'learning_rate' in log]\n",
    "        \n",
    "        # Create plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(steps, loss, label='Training Loss')\n",
    "        ax1.set_xlabel('Step')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Learning rate plot\n",
    "        ax2.plot(steps, lr, label='Learning Rate', color='green')\n",
    "        ax2.set_xlabel('Step')\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax2.set_title('Learning Rate Schedule')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print current stats\n",
    "        if steps:\n",
    "            print(f\"Current step: {steps[-1]}/{state.max_steps}\")\n",
    "            print(f\"Current loss: {loss[-1]:.4f}\")\n",
    "            print(f\"Current learning rate: {lr[-1]:.8f}\")\n",
    "            \n",
    "            # Calculate training speed\n",
    "            if len(steps) > 10:\n",
    "                recent_steps = steps[-10:]\n",
    "                if len(recent_steps) > 1 and state.max_steps:\n",
    "                    steps_per_log = recent_steps[1] - recent_steps[0]\n",
    "                    remaining_steps = state.max_steps - steps[-1]\n",
    "                    steps_to_go = max(0, remaining_steps)\n",
    "                    estimated_logs = steps_to_go / steps_per_log if steps_per_log else 0\n",
    "                    print(f\"Approximately {estimated_logs:.0f} logs remaining\")\n",
    "\n",
    "# Setup training with proper logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,    # Evaluate every 100 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,    # Save model every 100 steps\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,    # Set a specific number of steps\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard to avoid conflicts\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create the trainer with our callback\n",
    "progress_callback = ProgressVisualizationCallback()\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[progress_callback],  # Add the callback here\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
