// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/lang.regexp;
import ballerinax/ai.agent;

isolated function getNumbers(string prompt) returns string[] {
    regexp:Span[] spans = re `-?\d+\.?\d*`.findAll(prompt);
    return spans.'map(span => span.substring());
}

isolated function getAnswer(string prompt) returns string {
    var result = re `.*(Answer is: .*)\n?`.findGroups(prompt);
    if result is () || result.length() <= 1 {
        return "Sorry! I don't know the answer";
    }
    var answer = result[1];
    return answer is () ? "Sorry! I don't know the answer" : answer.substring();
}

isolated function getDecimals(string[] numbers) returns decimal[] {
    decimal[] decimalVals = [];
    foreach var num in numbers {
        decimal|error decimalVal = decimal:fromString(num);
        decimalVals.push(decimalVal is decimal ? decimalVal : 0d);
    }
    return decimalVals;
}

isolated function getInt(string number) returns int {
    int|error intVal = int:fromString(number);
    return intVal is int ? intVal : 0;
}

type MockLlmToolCall record {|
    string action;
    json action_input;
|};

@agent:Tool
isolated function sum(decimal[] numbers) returns string {
    decimal total = 0;
    foreach decimal number in numbers {
        total += number;
    }
    return string `Answer is: ${total}`;
}

@agent:Tool
isolated function mutiply(int a, int b) returns string {
    return string `Answer is: ${a * b}`;
}

@agent:Tool
isolated function getEmails() returns stream<Mail, agent:Error?>|error? {
    return [{body: "Mail Body 1"}, {body: "Mail Body 2"}, {body: "Mail Body 3"}].toStream();
}

isolated client distinct class MockLlm {
    *agent:Model;

    isolated remote function chat(agent:ChatMessage[] messages, agent:ChatCompletionFunctions[] tools, string? stop)
        returns agent:ChatAssistantMessage[]|agent:LlmError {
        agent:ChatMessage lastMessage = messages.pop();
        string query = lastMessage is agent:ChatUserMessage|agent:ChatFunctionMessage ? lastMessage.content ?: "" : "";
        if query.includes("Mail Body") {
            MockLlmToolCall toolCall = {action: "Final answer", action_input: query};
            return getChatAssistantMessages(string `Answer is:  ${toolCall.toJsonString()})`);
        }
        if query.includes("Answer is:") {
            MockLlmToolCall toolCall = {action: "Final answer", action_input: getAnswer(query)};
            return getChatAssistantMessages(string `Answer is:  ${toolCall.toJsonString()})`);
        }
        if query.toLowerAscii().includes("mail") {
            MockLlmToolCall toolCall = {action: "getEmails", action_input: {}};
            return getChatAssistantMessages(string `I need to call the searchDoc tool. Action: ${toolCall.toJsonString()}`);
        }
        if query.toLowerAscii().includes("search") {
            regexp:Span? span = re `'.*'`.find(query);
            string searchQuery = span is () ? "No search query" : span.substring();
            MockLlmToolCall toolCall = {action: "searchDoc", action_input: {searchQuery}};
            return getChatAssistantMessages(string `I need to call the searchDoc tool. Action: ${toolCall.toJsonString()}`);
        }
        if query.toLowerAscii().includes("sum") || query.toLowerAscii().includes("add") {
            decimal[] numbers = getDecimals(getNumbers(query));
            MockLlmToolCall toolCall = {action: "sum", action_input: {numbers}};
            return getChatAssistantMessages(string `I need to call the sum tool. Action: ${toolCall.toJsonString()}`);
        }
        if query.toLowerAscii().includes("mult") || query.toLowerAscii().includes("prod") {
            string[] numbers = getNumbers(query);
            int a = getInt(numbers.shift());
            int b = getInt(numbers.shift());
            MockLlmToolCall toolCall = {action: "mutiply", action_input: {a, b}};
            return getChatAssistantMessages(string `I need to call the sum tool. Action: ${toolCall.toJsonString()}`);
        }
        return error agent:LlmError("I can't understand");
    }
}

isolated function getChatAssistantMessages(string content) returns agent:ChatAssistantMessage[] {
    return [{role: agent:ASSISTANT, content}];
}

final MockLlm model = new;
final agent:Agent agent = check new (model = model,
    systemPrompt = {role: "Math tutor", instructions: "Help the students with their questions."},
    tools = [sum, mutiply, new SearchToolKit(), getEmails], agentType = agent:REACT_AGENT
);

isolated class SearchToolKit {
    *agent:BaseToolKit;

    public isolated function getTools() returns agent:ToolConfig[] {
        return agent:getToolConfigs([self.searchDoc]);
    }

    @agent:Tool
    public isolated function searchDoc(string searchQuery) returns string {
        return string `Answer is: No result found on doc for ${searchQuery}`;
    }
}

// Copyright (c) 2023 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ai.agent.mistral;

import ballerina/http;
import ballerina/lang.regexp;
import ballerina/uuid;
import ballerinax/azure.openai.chat as azure_chat;
import ballerinax/openai.chat;

// TODO: change the configs to extend the config record from the respective clients.
// requirs using never prompt?; never stop? to prevent setting those during initialization
// change once https://github.com/ballerina-platform/ballerina-lang/issues/32012 is fixed

# Roles for the chat messages.
public enum ROLE {
    SYSTEM = "system",
    USER = "user",
    ASSISTANT = "assistant",
    FUNCTION = "function"
}

# Model types for OpenAI
@display {label: "OpenAI Model Names"}
public enum OPEN_AI_MODEL_NAMES {
    O3_MINI = "o3-mini",
    O3_MINI_2025_01_31 = "o3-mini-2025-01-31",
    O1 = "o1",
    O1_2024_12_17 = "o1-2024-12-17",
    GPT_4O = "gpt-4o",
    GPT_4O_2024_11_20 = "gpt-4o-2024-11-20",
    GPT_4O_2024_08_06 = "gpt-4o-2024-08-06",
    GPT_4O_2024_05_13 = "gpt-4o-2024-05-13",
    GPT_4O_MINI = "gpt-4o-mini",
    GPT_4O_MINI_2024_07_18 = "gpt-4o-mini-2024-07-18",
    GPT_4_TURBO = "gpt-4-turbo",
    GPT_4_TURBO_2024_04_09 = "gpt-4-turbo-2024-04-09",
    GPT_4_0125_PREVIEW = "gpt-4-0125-preview",
    GPT_4_TURBO_PREVIEW = "gpt-4-turbo-preview",
    GPT_4_1106_PREVIEW = "gpt-4-1106-preview",
    GPT_4_VISION_PREVIEW = "gpt-4-vision-preview",
    GPT_4 = "gpt-4",
    GPT_4_0314 = "gpt-4-0314",
    GPT_4_0613 = "gpt-4-0613",
    GPT_4_32K = "gpt-4-32k",
    GPT_4_32K_0314 = "gpt-4-32k-0314",
    GPT_4_32K_0613 = "gpt-4-32k-0613",
    GPT_3_5_TURBO = "gpt-3.5-turbo",
    GPT_3_5_TURBO_16K = "gpt-3.5-turbo-16k",
    GPT_3_5_TURBO_0301 = "gpt-3.5-turbo-0301",
    GPT_3_5_TURBO_0613 = "gpt-3.5-turbo-0613",
    GPT_3_5_TURBO_1106 = "gpt-3.5-turbo-1106",
    GPT_3_5_TURBO_0125 = "gpt-3.5-turbo-0125",
    GPT_3_5_TURBO_16K_0613 = "gpt-3.5-turbo-16k-0613"
}

# Models types for Anthropic
@display {label: "Anthropic Model Names"}
public enum ANTHROPIC_MODEL_NAMES {
    CLAUDE_3_7_SONNET_20250219 = "claude-3-7-sonnet-20250219",
    CLAUDE_3_5_HAIKU_20241022 = "claude-3-5-haiku-20241022",
    CLAUDE_3_5_SONNET_20241022 = "claude-3-5-sonnet-20241022",
    CLAUDE_3_5_SONNET_20240620 = "claude-3-5-sonnet-20240620",
    CLAUDE_3_OPUS_20240229 = "claude-3-opus-20240229",
    CLAUDE_3_SONNET_20240229 = "claude-3-sonnet-20240229",
    CLAUDE_3_HAIKU_20240307 = "claude-3-haiku-20240307"
}

# Models types for Mistral AI
@display {label: "Mistral AI Model Names"}
public enum MISTRAL_AI_MODEL_NAMES {
    MINISTRAL_3B_2410 = "ministral-3b-2410",
    MINISTRAL_8B_2410 = "ministral-8b-2410",
    OPEN_MISTRAL_7B = "open-mistral-7b",
    OPEN_MISTRAL_NEMO = "open-mistral-nemo",
    OPEN_MIXTRAL_8X7B = "open-mixtral-8x7b",
    OPEN_MIXTRAL_8X22B = "open-mixtral-8x22b",
    MISTRAL_SMALL_2402 = "mistral-small-2402",
    MISTRAL_SMALL_2409 = "mistral-small-2409",
    MISTRAL_SMALL_2501 = "mistral-small-2501",
    MISTRAL_MEDIUM_2312 = "mistral-medium-2312",
    MISTRAL_LARGE_2402 = "mistral-large-2402",
    MISTRAL_LARGE_2407 = "mistral-large-2407",
    MISTRAL_LARGE_2411 = "mistral-large-2411",
    PIXTRAL_LARGE_2411 = "pixtral-large-2411",
    CODESTRAL_2405 = "codestral-2405",
    CODESTRAL_2501 = "codestral-2501",
    CODESTRAL_MAMBA_2407 = "codestral-mamba-2407",
    PIXTRAL_12B_2409 = "pixtral-12b-2409",
    MISTRAL_SABA_2502 = "mistral-saba-2502",
    MISTRAL_SMALL_MODEL = "mistral-small-latest",
    MISTRAL_MEDIUM_MODEL = "mistral-medium-latest",
    MISTRAL_LARGE_MODEL = "mistral-large-latest"
}

# Configurations for controlling the behaviours when communicating with a remote HTTP endpoint.
@display {label: "Connection Configuration"}
public type ConnectionConfig record {|

    # The HTTP version understood by the client
    @display {label: "HTTP Version"}
    http:HttpVersion httpVersion = http:HTTP_2_0;

    # Configurations related to HTTP/1.x protocol
    @display {label: "HTTP1 Settings"}
    http:ClientHttp1Settings http1Settings?;

    # Configurations related to HTTP/2 protocol
    @display {label: "HTTP2 Settings"}
    http:ClientHttp2Settings http2Settings?;

    # The maximum time to wait (in seconds) for a response before closing the connection
    @display {label: "Timeout"}
    decimal timeout = 60;

    # The choice of setting `forwarded`/`x-forwarded` header
    @display {label: "Forwarded"}
    string forwarded = "disable";

    # Configurations associated with request pooling
    @display {label: "Pool Configuration"}
    http:PoolConfiguration poolConfig?;

    # HTTP caching related configurations
    @display {label: "Cache Configuration"}
    http:CacheConfig cache?;

    # Specifies the way of handling compression (`accept-encoding`) header
    @display {label: "Compression"}
    http:Compression compression = http:COMPRESSION_AUTO;

    # Configurations associated with the behaviour of the Circuit Breaker
    @display {label: "Circuit Breaker Configuration"}
    http:CircuitBreakerConfig circuitBreaker?;

    # Configurations associated with retrying
    @display {label: "Retry Configuration"}
    http:RetryConfig retryConfig?;

    # Configurations associated with inbound response size limits
    @display {label: "Response Limit Configuration"}
    http:ResponseLimitConfigs responseLimits?;

    # SSL/TLS-related options
    @display {label: "Secure Socket Configuration"}
    http:ClientSecureSocket secureSocket?;

    # Proxy server related options
    @display {label: "Proxy Configuration"}
    http:ProxyConfig proxy?;

    # Enables the inbound payload validation functionality which provided by the constraint package. Enabled by default
    @display {label: "Payload Validation"}
    boolean validation = true;
|};

# User chat message record.
public type ChatUserMessage record {|
    # Role of the message
    USER role;
    # Content of the message
    string content;
    # An optional name for the participant
    # Provides the model information to differentiate between participants of the same role
    string name?;
|};

# System chat message record.
public type ChatSystemMessage record {|
    # Role of the message
    SYSTEM role;
    # Content of the message
    string content;
    # An optional name for the participant
    # Provides the model information to differentiate between participants of the same role
    string name?;
|};

# Assistant chat message record.
public type ChatAssistantMessage record {|
    # Role of the message
    ASSISTANT role;
    # The contents of the assistant message
    # Required unless `tool_calls` or `function_call` is specified
    string? content = ();
    # An optional name for the participant
    # Provides the model information to differentiate between participants of the same role
    string name?;
    # The function calls generated by the model, such as function calls
    FunctionCall? function_call = ();
|};

# Function message record.
public type ChatFunctionMessage record {|
    # Role of the message
    FUNCTION role;
    # Content of the message
    string? content = ();
    # Name of the function when the message is a function call
    string name;
    # Identifier for the tool call
    string id?;
|};

# Chat message record.
public type ChatMessage ChatUserMessage|ChatSystemMessage|ChatAssistantMessage|ChatFunctionMessage;

# Mistral message record.
type MistralMessages mistral:AssistantMessage|mistral:SystemMessage|mistral:UserMessage|mistral:ToolMessage;

# Function definitions for function calling API.
public type ChatCompletionFunctions record {|
    # Name of the function
    string name;
    # Description of the function
    string description;
    # Parameters of the function
    JsonInputSchema parameters?;
|};

# Function call record
public type FunctionCall record {|
    # Name of the function
    string name;
    # Arguments of the function
    string arguments;
    # Identifier for the tool call
    string id?;
|};

# Anthropic API request message format
type AnthropicMessage record {|
    # Role of the participant in the conversation (e.g., "user" or "assistant")
    string role;
    # The message content
    string content;
|};

# Anthropic API response format
type AnthropicApiResponse record {|
    # Unique identifier for the response message
    string id;
    # The Anthropic model used for generating the response
    string model;
    # The type of the response (e.g., "message")
    string 'type;
    # Array of content blocks containing the response text and media
    ContentBlock[] content;
    # Role of the message sender (typically "assistant")
    string role;
    # Reason why the generation stopped (e.g., "end_turn", "max_tokens")
    string stop_reason;
    # The sequence that caused generation to stop, if applicable
    string? stop_sequence;
    # Token usage statistics for the request and response
    Usage usage;
|};

# Content block in Anthropic API response
type ContentBlock record {|
    # The type of content (e.g., "text" or "tool_use")
    string 'type;
    # The actual text content (for text type)
    string text?;
    # Tool use information (for tool_use type)
    string id?;
    # Name of the tool being used
    string name?;
    # Input parameters for the tool
    json input?;
|};

# Usage statistics in Anthropic API response
type Usage record {|
    # Number of tokens in the input messages
    int input_tokens;
    # Number of tokens in the generated response
    int output_tokens;
    # Number of input tokens used for cache creation, if applicable
    int? cache_creation_input_tokens = ();
    # Number of input tokens read from cache, if applicable
    int? cache_read_input_tokens = ();
|};

# Anthropic Tool definition
type AnthropicTool record {|
    # Name of the tool
    string name;
    # Description of the tool
    string description;
    # Input schema of the tool in JSON Schema format
    json input_schema;
|};

# Represents an extendable client for interacting with an AI model.
public type Model distinct isolated client object {
    # Sends a chat request to the model with the given messages and tools.
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Function to be called, chat response or an error in-case of failures
    isolated remote function chat(ChatMessage[] messages, ChatCompletionFunctions[] tools = [], string? stop = ())
        returns ChatAssistantMessage[]|LlmError;
};

# OpenAiModel is a client class that provides an interface for interacting with OpenAI language models.
public isolated client class OpenAiModel {
    *Model;
    private final chat:Client llmClient;
    private final string modelType;

    # Initializes the OpenAI model with the given connection configuration and model configuration.
    #
    # + apiKey - The OpenAI API key
    # + modelType - The OpenAI model name
    # + serviceUrl - The base URL of OpenAI API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output  
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `Error`
    public isolated function init(@display {label: "API Key"} string apiKey,
            @display {label: "Model Type"} OPEN_AI_MODEL_NAMES modelType,
            @display {label: "Service URL"} string serviceUrl = OPENAI_SERVICE_URL,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig) returns Error? {
        chat:ClientHttp1Settings?|error http1Settings = connectionConfig?.http1Settings.cloneWithType();
        if http1Settings is error {
            return error Error("Failed to clone http1Settings", http1Settings);
        }
        chat:ConnectionConfig openAiConfig = {
            auth: {
                token: apiKey
            },
            httpVersion: connectionConfig.httpVersion,
            http1Settings: http1Settings,
            http2Settings: connectionConfig.http2Settings,
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig.poolConfig,
            cache: connectionConfig.cache,
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig.circuitBreaker,
            retryConfig: connectionConfig.retryConfig,
            responseLimits: connectionConfig.responseLimits,
            secureSocket: connectionConfig.secureSocket,
            proxy: connectionConfig.proxy,
            validation: connectionConfig.validation
        };
        chat:Client|error llmClient = new (openAiConfig);
        if llmClient is error {
            return error Error("Failed to initialize OpenAiModel", llmClient);
        }
        self.llmClient = llmClient;
        self.modelType = modelType;
    }

    # Sends a chat request to the OpenAI model with the given messages and tools.
    #
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Function to be called, chat response or an error in-case of failures
    isolated remote function chat(ChatMessage[] messages, ChatCompletionFunctions[] tools, string? stop = ())
        returns ChatAssistantMessage[]|LlmError {
        chat:CreateChatCompletionRequest request = {model: self.modelType, stop, messages};
        if tools.length() > 0 {
            request.functions = tools;
        }
        chat:CreateChatCompletionResponse|error response = self.llmClient->/chat/completions.post(request);
        if response is error {
            return error LlmConnectionError("Error while connecting to the model", response);
        }
        chat:CreateChatCompletionResponse_choices[] choices = response.choices;
        ChatAssistantMessage[] chatAssistantMessages = [];
        foreach chat:CreateChatCompletionResponse_choices choice in choices {
            chat:ChatCompletionResponseMessage? message = choice.message;
            string? content = message?.content;
            if content is string {
                chatAssistantMessages.push({role: ASSISTANT, content});
            }
            chat:ChatCompletionRequestAssistantMessage_function_call? function_call = message?.function_call;
            if function_call is chat:ChatCompletionRequestAssistantMessage_function_call {
                chatAssistantMessages.push({role: ASSISTANT, function_call: {name: function_call.name, arguments: function_call.arguments}});
            }
        }
        return chatAssistantMessages.length() > 0 ? chatAssistantMessages
            : error LlmInvalidResponseError("Empty response from the model when using function call API");
    }

}

# AzureOpenAiModel is a client class that provides an interface for interacting with Azure-hosted OpenAI language models.
public isolated client class AzureOpenAiModel {
    *Model;
    private final azure_chat:Client llmClient;
    private final string deploymentId;
    private final string apiVersion;

    # Initializes the Azure OpenAI model with the given connection configuration and model configuration.
    #
    # + serviceUrl - The base URL of Azure OpenAI API endpoint
    # + apiKey - The Azure OpenAI API key
    # + deploymentId - The deployment identifier for the specific model deployment in Azure  
    # + apiVersion - The Azure OpenAI API version (e.g., "2023-07-01-preview")
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output  
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `Error`
    public isolated function init(@display {label: "Service URL"} string serviceUrl,
            @display {label: "API Key"} string apiKey,
            @display {label: "Deployment ID"} string deploymentId,
            @display {label: "API Version"} string apiVersion,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig) returns Error? {

        azure_chat:ClientHttp1Settings?|error http1Settings = connectionConfig?.http1Settings.cloneWithType();
        if http1Settings is error {
            return error Error("Failed to clone http1Settings", http1Settings);
        }
        // Merge your local connection config with the required auth config
        azure_chat:ConnectionConfig azureAiConfig = {
            auth: {apiKey},
            httpVersion: connectionConfig.httpVersion,
            http1Settings: http1Settings,
            http2Settings: connectionConfig.http2Settings,
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig.poolConfig,
            cache: connectionConfig.cache,
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig.circuitBreaker,
            retryConfig: connectionConfig.retryConfig,
            responseLimits: connectionConfig.responseLimits,
            secureSocket: connectionConfig.secureSocket,
            proxy: connectionConfig.proxy,
            validation: connectionConfig.validation
        };
        azure_chat:Client|error llmClient = new (azureAiConfig, serviceUrl);
        if llmClient is error {
            return error Error("Failed to initialize AzureOpenAiModel", llmClient);
        }
        self.llmClient = llmClient;
        self.deploymentId = deploymentId;
        self.apiVersion = apiVersion;
    }

    # Sends a chat request to the OpenAI model with the given messages and tools.
    #
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Function to be called, chat response or an error in-case of failures
    isolated remote function chat(ChatMessage[] messages, ChatCompletionFunctions[] tools, string? stop = ()) returns ChatAssistantMessage[]|LlmError {
        azure_chat:CreateChatCompletionRequest request = {stop, messages};
        if tools.length() > 0 {
            request.functions = tools;
        }
        azure_chat:CreateChatCompletionResponse|error response =
            self.llmClient->/deployments/[self.deploymentId]/chat/completions.post(self.apiVersion, request);
        if response is error {
            return error LlmConnectionError("Error while connecting to the model", response);
        }

        record {|
            azure_chat:ChatCompletionResponseMessage message?;
            azure_chat:ContentFilterChoiceResults content_filter_results?;
            int index?;
            string finish_reason?;
            anydata...;
        |}[]? choices = response.choices;

        LlmInvalidResponseError invalidResponseError = error LlmInvalidResponseError("Empty response from the model when using function call API");
        if choices is () {
            return invalidResponseError;
        }
        ChatAssistantMessage[] chatAssistantMessages = [];
        foreach var choice in choices {
            azure_chat:ChatCompletionResponseMessage? message = choice.message;
            string? content = message?.content;
            if content is string {
                // check whether the model response is text
                chatAssistantMessages.push({role: ASSISTANT, content});
            }
            azure_chat:ChatCompletionFunctionCall? function_call = message?.function_call;
            if function_call is chat:ChatCompletionRequestAssistantMessage_function_call {
                chatAssistantMessages.push({role: ASSISTANT, function_call: {name: function_call.name, arguments: function_call.arguments}});
            }
        }
        return chatAssistantMessages.length() > 0 ? chatAssistantMessages
            : invalidResponseError;
    }
}

# AnthropicModel is a client class that provides an interface for interacting with Anthropic language models.
public isolated client class AnthropicModel {
    *Model;
    private final http:Client AnthropicClient;
    private final string apiKey;
    private final string modelType;
    private final string apiVersion;
    private final int maxTokens;

    # Initializes the Anthropic model with the given connection configuration and model configuration.
    #
    # + apiKey - The Anthropic API key
    # + modelType - The Anthropic model name
    # + apiVersion - The Anthropic API version (e.g., "2023-06-01")  
    # + serviceUrl - The base URL of Anthropic API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output  
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `Error`
    public isolated function init(@display {label: "API Key"} string apiKey,
            @display {label: "Model Type"} ANTHROPIC_MODEL_NAMES modelType,
            @display {label: "API Version"} string apiVersion,
            @display {label: "Service URL"} string serviceUrl = ANTHROPIC_SERVICE_URL,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig) returns Error? {

        // Convert ConnectionConfig to http:ClientConfiguration
        http:ClientConfiguration anthropicConfig = {
            httpVersion: connectionConfig.httpVersion,
            http1Settings: connectionConfig.http1Settings ?: {},
            http2Settings: connectionConfig?.http2Settings ?: {},
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig?.poolConfig,
            cache: connectionConfig?.cache ?: {},
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig?.circuitBreaker,
            retryConfig: connectionConfig?.retryConfig,
            responseLimits: connectionConfig?.responseLimits ?: {},
            secureSocket: connectionConfig?.secureSocket,
            proxy: connectionConfig?.proxy,
            validation: connectionConfig.validation
        };

        http:Client|error httpClient = new http:Client(serviceUrl, anthropicConfig);

        if (httpClient is error) {
            return error Error("Failed to initialize Anthropic Model", httpClient);
        }

        self.AnthropicClient = httpClient;
        self.apiKey = apiKey;
        self.modelType = modelType;
        self.apiVersion = apiVersion;
        self.maxTokens = maxTokens;
    }

    # Converts standard ChatMessage array to Anthropic's message format
    #
    # + messages - List of chat messages 
    # + return - return value description
    private isolated function mapToAnthropicMessages(ChatMessage[] messages) returns AnthropicMessage[] {
        AnthropicMessage[] anthropicMessages = [];

        foreach ChatMessage message in messages {
            if message is ChatUserMessage {
                anthropicMessages.push({
                    role: USER,
                    content: message.content
                });
            } else if message is ChatSystemMessage {
                // Add a user message containing the system prompt
                anthropicMessages.push({
                    role: USER,
                    content: string `<system>${message.content}</system>\n\n`
                });
            } else if message is ChatAssistantMessage && message.content is string {
                anthropicMessages.push({
                    role: ASSISTANT,
                    content: message.content ?: ""
                });
            } else if message is ChatFunctionMessage && message.content is string {
                // Include function results as user messages with special formatting
                anthropicMessages.push({
                    role: USER,
                    content: string `<function_results>\nFunction: ${message.name}\nOutput: ${message.content ?: ""}\n</function_results>`
                });
            }
        }
        return anthropicMessages;
    }

    # Maps ChatCompletionFunctions to Anthropic's tool format
    #
    # + tools - Array of tool definitions
    # + return - Array of Anthropic tool definitions
    private isolated function mapToAnthropicTools(ChatCompletionFunctions[] tools) returns AnthropicTool[] {
        AnthropicTool[] anthropicTools = [];

        foreach ChatCompletionFunctions tool in tools {
            JsonInputSchema schema = tool.parameters ?: {'type: "object", properties: {}};

            // Create Anthropic tool with input_schema instead of parameters
            AnthropicTool AnthropicTool = {
                name: tool.name,
                description: tool.description,
                input_schema: schema
            };

            anthropicTools.push(AnthropicTool);
        }

        return anthropicTools;
    }

    # Uses Anthropic API to generate a response
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion (not used in this implementation)
    # + return - Chat response or an error in case of failures
    isolated remote function chat(ChatMessage[] messages, ChatCompletionFunctions[] tools = [], string? stop = ())
        returns ChatAssistantMessage[]|LlmError {

        // Map messages to Anthropic format
        AnthropicMessage[] anthropicMessages = self.mapToAnthropicMessages(messages);

        // Prepare request payload
        map<json> requestPayload = {
            "model": self.modelType,
            "max_tokens": self.maxTokens,
            "messages": anthropicMessages
        };

        if stop !is () {
            requestPayload["stop_sequences"] = [stop];
        }

        // If tools are provided, add them to the request
        if tools.length() > 0 {
            AnthropicTool[] anthropicTools = self.mapToAnthropicTools(tools);
            requestPayload["tools"] = anthropicTools;
        }

        // Send request to Anthropic API with proper headers
        map<string> headers = {
            "x-api-key": self.apiKey,
            "anthropic-version": self.apiVersion,
            "content-type": "application/json"
        };

        AnthropicApiResponse|error anthropicResponse = self.AnthropicClient->/messages.post(requestPayload, headers);

        if anthropicResponse is error {
            return error LlmInvalidResponseError("Unexpected response format from Anthropic API", anthropicResponse);
        }

        string responseText = "";
        FunctionCall[] functionCalls = [];

        ContentBlock[] contentBlocks = anthropicResponse.content;

        foreach ContentBlock block in contentBlocks {
            string blockType = block.'type;
            if blockType == "tool_use" {
                string blockName = block.name ?: "";
                json inputJson = block?.input;
                functionCalls.push({
                    name: blockName,
                    arguments: inputJson.toJsonString()
                });
            } else if blockType == "text" {
                responseText = block.text ?: "";
            }
        }

        // Return response with function calls first, then text
        if functionCalls.length() > 0 {
            ChatAssistantMessage[] returnResponse = [];

            // First add each function call as a separate message
            foreach FunctionCall call in functionCalls {
                returnResponse.push({
                    role: ASSISTANT,
                    function_call: call
                });
            }

            // Then add the text response if it exists 
            if responseText != "" {
                returnResponse.push({
                    role: ASSISTANT,
                    content: responseText
                });
            }

            return returnResponse;
        } else {
            return [
                {
                    role: ASSISTANT,
                    content: responseText
                }
            ];
        }
    }
}

# MistralAiModel is a client class that provides an interface for interacting with Mistral AI language models.
public isolated client class MistralAiModel {
    *Model;
    private final mistral:Client llmClient;
    private final string modelType;
    private final string apiKey;

    # # Initializes the Mistral AI model with the given connection configuration and model configuration.
    #
    # + apiKey - The Mistral AI API key
    # + modelType - The Mistral AI model name
    # + serviceUrl - The base URL of Mistral AI API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `Error`
    public isolated function init(@display {label: "API Key"} string apiKey,
            @display {label: "Model Type"} MISTRAL_AI_MODEL_NAMES modelType,
            @display {label: "Service URL"} string serviceUrl = MISTRAL_AI_SERVICE_URL,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig
    ) returns Error? {

        mistral:ConnectionConfig mistralConfig = {
            auth: {token: apiKey},
            httpVersion: connectionConfig.httpVersion,
            http1Settings: connectionConfig.http1Settings ?: {},
            http2Settings: connectionConfig?.http2Settings ?: {},
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig?.poolConfig,
            cache: connectionConfig?.cache ?: {},
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig?.circuitBreaker,
            retryConfig: connectionConfig?.retryConfig,
            responseLimits: connectionConfig?.responseLimits ?: {},
            secureSocket: connectionConfig?.secureSocket,
            proxy: connectionConfig?.proxy,
            validation: connectionConfig.validation
        };

        mistral:Client|error llmClient = new (mistralConfig);
        if llmClient is error {
            return error Error("Failed to initialize MistralAiModel", llmClient);
        }

        self.llmClient = llmClient;
        self.modelType = modelType;
        self.apiKey = apiKey;
    }

    # Uses function call API to determine next function to be called
    #
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Returns an array of ChatAssistantMessage or an LlmError in case of failures
    isolated remote function chat(ChatMessage[] messages, ChatCompletionFunctions[] tools, string? stop = ())
    returns ChatAssistantMessage[]|LlmError {
        MistralMessages[] mistralMessages = check self.mapMistralMessageRecords(messages);
        mistral:ChatCompletionRequest request = {model: self.modelType, stop, messages: mistralMessages};

        if tools.length() > 0 {
            mistral:Function[] mistralFunctions = [];
            foreach ChatCompletionFunctions toolFunction in tools {
                mistral:Function mistralFunction = {
                    name: toolFunction.name,
                    description: toolFunction.description,
                    strict: false,
                    parameters: toolFunction.parameters ?: {}
                };
                mistralFunctions.push(mistralFunction);
            }

            mistral:Tool[] mistralTools = [];
            foreach mistral:Function mistralfunction in mistralFunctions {
                mistral:Tool mistralTool = {'function: mistralfunction};
                mistralTools.push(mistralTool);
            }
            request.tools = mistralTools;
        }

        mistral:ChatCompletionResponse|error response = self.llmClient->/chat/completions.post(request);

        if response is error {
            return error LlmConnectionError("Error while connecting to the model", response);
        }

        ChatAssistantMessage[] chatAssistantMessage = check self.getAssistantMessage(response);
        return chatAssistantMessage;
    }

    # Generates a random tool ID.
    #
    # + return - A random tool ID string
    private isolated function generateToolId() returns string {
        string randomToolId = "";
        string randomId = uuid:createRandomUuid();
        regexp:RegExp alphanumericPattern = re `[a-zA-Z0-9]`;
        int iterationCount = 0;

        foreach string character in randomId {
            if alphanumericPattern.isFullMatch(character) {
                randomToolId = randomToolId + character;
                iterationCount = iterationCount + 1;
            }
            if iterationCount == 9 {
                break;
            }
        }
        return randomToolId;
    }

    # Maps an array of `ChatMessage` records to corresponding Mistral message records.
    #
    # + messages - Array of chat messages to be converted
    # + return - An `LlmError` or an array of Mistral message records
    private isolated function mapMistralMessageRecords(ChatMessage[] messages) returns MistralMessages[] {
        MistralMessages[] mistralMessages = [];
        foreach ChatMessage message in messages {
            if message is ChatUserMessage {
                mistral:UserMessage usermessage = {
                    role: USER,
                    content: message.content
                };
                mistralMessages.push(usermessage);
            } else if message is ChatSystemMessage {
                mistral:SystemMessage systemMessage = {
                    role: SYSTEM,
                    content: message.content
                };
                mistralMessages.push(systemMessage);
            } else if message is ChatAssistantMessage {
                if message.function_call is () {
                    mistral:AssistantMessage mistralAssistantMessage = {
                        role: ASSISTANT,
                        content: message.content,
                        toolCalls: ()
                    };
                    mistralMessages.push(mistralAssistantMessage);
                } else {
                    mistral:FunctionCall functionCall = {
                        name: message.function_call is FunctionCall ? message.function_call?.name ?: "" : "",
                        arguments: message.function_call is FunctionCall ? message.function_call?.arguments ?: "" : ""
                    };
                    mistral:ToolCall[] toolCall = [
                        {
                            'function: functionCall,
                            id: message.function_call?.id ?: self.generateToolId()
                        }
                    ];
                    mistral:AssistantMessage mistralAssistantMessage = {
                        role: ASSISTANT,
                        content: message.content,
                        toolCalls: toolCall
                    };
                    mistralMessages.push(mistralAssistantMessage);

                }
            } else if message is ChatFunctionMessage {
                mistral:ToolMessage mistralToolMessage = {
                    role: "tool",
                    content: message.content,
                    toolCallId: message.id ?: self.generateToolId()
                };
                mistralMessages.push(mistralToolMessage);
            }
        }
        return mistralMessages;
    }

    # Extracts assistant messages from a Mistral chat completion response.
    #
    # + response - The response from LLM
    # + return - An array of ChatAssistantMessage records
    private isolated function getAssistantMessage(mistral:ChatCompletionResponse response) returns ChatAssistantMessage[]|LlmError {
        mistral:AssistantMessage message;
        mistral:ChatCompletionChoice[]? choices = response.choices;
        if choices !is () {
            message = choices[0].message;
            string|mistral:ContentChunk[]? content = message?.content;
            if content is string && content.length() > 0 {
                return [{role: ASSISTANT, content}];
            } else if message?.toolCalls !is () {
                mistral:ToolCall[]? toolCall = message?.toolCalls;
                if toolCall !is () {
                    foreach mistral:ToolCall toolcall in toolCall {
                        FunctionCall functionCall = {
                            name: toolcall.'function.name,
                            id: toolcall.id,
                            arguments: toolcall.'function.arguments.toString()
                        };
                        return [({role: ASSISTANT, function_call: functionCall})];
                    }
                }
            } else if content is mistral:TextChunk[] {
                string contentString = "";
                foreach mistral:ContentChunk chunk in content {
                    contentString = contentString + chunk.text;
                }
                return [{role: ASSISTANT, content: contentString}];
            } else if content is () {
                return error LlmError("Empty response from the model when using function call API", cause = content);
            } else if content is mistral:ImageURLChunk[]|mistral:DocumentURLChunk[]|mistral:ReferenceChunk[] {
                return error LlmError("Unsupported content type", cause = content);
            }
        }
        return error LlmInvalidResponseError("Empty response from the model when using function call API");
    }
}

// Copyright (c) 2018 WSO2 Inc. (http://www.wso2.org) All Rights Reserved.
//
// WSO2 Inc. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/crypto;
import ballerina/time;

# Represents JWT issuer configurations.
#
# + issuer - JWT issuer, which is mapped to the `iss`
# + username - JWT username, which is mapped to the `sub`
# + audience - JWT audience, which is mapped to the `aud`
# + jwtId - JWT ID, which is mapped to the `jti`
# + keyId - JWT key ID, which is mapped the `kid`
# + customClaims - Map of custom claims
# + expTime - Expiry time in seconds
# + signatureConfig - JWT signature configurations
public type IssuerConfig record {|
    string issuer?;
    string username?;
    string|string[] audience?;
    string jwtId?;
    string keyId?;
    map<json> customClaims?;
    decimal expTime = 300;
    IssuerSignatureConfig signatureConfig?;
|};

# Represents JWT signature configurations.
#
# + algorithm - Cryptographic signing algorithm for JWS
# + config - KeyStore configurations, private key configurations, `crypto:PrivateKey` or shared key configurations
public type IssuerSignatureConfig record {|
    SigningAlgorithm algorithm = RS256;
    record {|
        crypto:KeyStore keyStore;
        string keyAlias;
        string keyPassword;
    |} | record {|
        string keyFile;
        string keyPassword?;
    |} | crypto:PrivateKey | string config?;
|};

# Issues a JWT based on the provided configurations. JWT will be signed (JWS) if `crypto:KeyStore` information is
# provided in the `jwt:KeyStoreConfig` and the `jwt:SigningAlgorithm` is not `jwt:NONE`.
# ```ballerina
# string jwt = check jwt:issue(issuerConfig);
# ```
#
# + issuerConfig - JWT issuer configurations
# + return - JWT as a `string` or else a `jwt:Error` if an error occurred
public isolated function issue(IssuerConfig issuerConfig) returns string|Error {
    Header header = prepareHeader(issuerConfig);
    Payload payload = preparePayload(issuerConfig);
    string headerString = check buildHeaderString(header);
    string payloadString = check buildPayloadString(payload);
    string jwtAssertion = headerString + "." + payloadString;

    IssuerSignatureConfig? issuerSignatureConfig = issuerConfig?.signatureConfig;
    if issuerSignatureConfig is () {
        return jwtAssertion;
    }
    IssuerSignatureConfig signatureConfig = <IssuerSignatureConfig>issuerSignatureConfig;
    SigningAlgorithm algorithm = signatureConfig.algorithm;
    if algorithm is NONE {
        return jwtAssertion;
    }
    var config = signatureConfig?.config;
    if config is () {
        return prepareError("Signing JWT requires keystore information or private key information.");
    } else if config is string {
        return hmacJwtAssertion(jwtAssertion, algorithm, config);
    } else if config?.keyStore is crypto:KeyStore {
        crypto:KeyStore keyStore = <crypto:KeyStore> config?.keyStore;
        string keyAlias = <string> config?.keyAlias;
        string keyPassword = <string> config?.keyPassword;
        crypto:PrivateKey|crypto:Error privateKey = crypto:decodeRsaPrivateKeyFromKeyStore(keyStore, keyAlias, keyPassword);
        if privateKey is crypto:PrivateKey {
            return signJwtAssertion(jwtAssertion, algorithm, privateKey);
        } else {
            return prepareError("Failed to decode private key.", privateKey);
        }
    } else if config is crypto:PrivateKey {
        return signJwtAssertion(jwtAssertion, algorithm, config);
    } else {
        string keyFile = <string> config?.keyFile;
        string? keyPassword = config?.keyPassword;
        crypto:PrivateKey|crypto:Error privateKey = crypto:decodeRsaPrivateKeyFromKeyFile(keyFile, keyPassword);
        if privateKey is crypto:PrivateKey {
            return signJwtAssertion(jwtAssertion, algorithm, privateKey);
        } else {
            return prepareError("Failed to decode private key.", privateKey);
        }
    }
}

isolated function signJwtAssertion(string jwtAssertion, SigningAlgorithm alg, crypto:PrivateKey privateKey)
                                   returns string|Error {
    match alg {
        RS256 => {
            byte[]|crypto:Error signature = crypto:signRsaSha256(jwtAssertion.toBytes(), privateKey);
            if signature is byte[] {
                return (jwtAssertion + "." + encodeBase64Url(signature));
            } else {
                return prepareError("RSA private key signing failed for SHA256 algorithm.", signature);
            }
        }
        RS384 => {
            byte[]|crypto:Error signature = crypto:signRsaSha384(jwtAssertion.toBytes(), privateKey);
            if signature is byte[] {
                return (jwtAssertion + "." + encodeBase64Url(signature));
            } else {
                return prepareError("RSA private key signing failed for SHA384 algorithm.", signature);
            }
        }
        RS512 => {
            byte[]|crypto:Error signature = crypto:signRsaSha512(jwtAssertion.toBytes(), privateKey);
            if signature is byte[] {
                return (jwtAssertion + "." + encodeBase64Url(signature));
            } else {
                return prepareError("RSA private key signing failed for SHA512 algorithm.", signature);
            }
        }
        _ => {
            return prepareError("Unsupported signing algorithm '" + alg.toString() + "'.");
        }
    }
}

isolated function hmacJwtAssertion(string jwtAssertion, SigningAlgorithm alg, string secret)
                                   returns string|Error {
    match alg {
        HS256 => {
            byte[]|crypto:Error signature = crypto:hmacSha256(jwtAssertion.toBytes(), secret.toBytes());
            if signature is byte[] {
                return (jwtAssertion + "." + encodeBase64Url(signature));
            } else {
                return prepareError("HMAC secret key signing failed for SHA256 algorithm.", signature);
            }
        }
        HS384 => {
            byte[]|crypto:Error signature = crypto:hmacSha384(jwtAssertion.toBytes(), secret.toBytes());
            if signature is byte[] {
                return (jwtAssertion + "." + encodeBase64Url(signature));
            } else {
                return prepareError("HMAC secret key signing failed for SHA384 algorithm.", signature);
            }
        }
        HS512 => {
            byte[]|crypto:Error signature = crypto:hmacSha512(jwtAssertion.toBytes(), secret.toBytes());
            if signature is byte[] {
                return (jwtAssertion + "." + encodeBase64Url(signature));
            } else {
                return prepareError("HMAC secret key signing failed for SHA512 algorithm.", signature);
            }
        }
        _ => {
            return prepareError("Unsupported signing algorithm '" + alg.toString() + "'.");
        }
    }
}

isolated function prepareHeader(IssuerConfig issuerConfig) returns Header {
    Header header = { alg: NONE, typ: "JWT" };
    IssuerSignatureConfig? issuerSignatureConfig = issuerConfig?.signatureConfig;
    if issuerSignatureConfig is IssuerSignatureConfig {
        header.alg = issuerSignatureConfig.algorithm;
    }
    string? kid = issuerConfig?.keyId;
    if kid is string {
        header.kid = kid;
    }
    return header;
}

isolated function preparePayload(IssuerConfig issuerConfig) returns Payload {
    [int, decimal] currentTime = time:utcNow();
    Payload payload = {
        exp: currentTime[0] + <int> issuerConfig.expTime,
        iat: currentTime[0],
        nbf: currentTime[0]
    };

    string? iss = issuerConfig?.issuer;
    if iss is string {
        payload.iss = iss;
    }
    string? sub = issuerConfig?.username;
    if sub is string {
        payload.sub = sub;
    }
    string|string[]? aud = issuerConfig?.audience;
    if aud is string || aud is string[] {
        payload.aud = aud;
    }
    string? jti = issuerConfig?.jwtId;
    if jti is string {
        payload.jti = jti;
    }

    map<json>? customClaims = issuerConfig?.customClaims;
    if customClaims is map<json> {
        foreach string key in customClaims.keys() {
            payload[key] = customClaims[key];
        }
    }
    return payload;
}

isolated function buildHeaderString(Header header) returns string|Error {
    if !validateMandatoryHeaderFields(header) {
        return prepareError("Mandatory field signing algorithm (alg) is empty.");
    }
    return encodeBase64Url(header.toJsonString().toBytes());
}

isolated function buildPayloadString(Payload payload) returns string|Error {
    return encodeBase64Url(payload.toJsonString().toBytes());
}

// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/os;
import ballerina/test;

configurable boolean isLiveServer = os:getEnv("IS_LIVE_SERVER") == "true";
configurable string token = isLiveServer ? os:getEnv("MISTRAL_API_KEY") : "test";
final string mockServiceUrl = "http://localhost:9090";
final Client mistralAiClient = check initClient();

function initClient() returns Client|error {
    if isLiveServer {
        return new ({auth: {token}});
    }
    return new ({auth: {token}}, mockServiceUrl);
}

@test:Config {
    groups: ["live_tests", "mock_tests"]
}
isolated function testChatCompletion() returns error? {

    UserMessage userMessage = {
        role: "user",
        content: "This is a test message"
    };

    ChatCompletionRequest chatRequest = {
        messages: [userMessage],
        model: "mistral-small-latest"
    };

    ChatCompletionResponse response = check mistralAiClient->/chat/completions.post(chatRequest);
    ChatCompletionChoice[]? choices = response.choices;
    if choices is ChatCompletionChoice[] {
        AssistantMessage? message = choices[0].message;
        string|ContentChunk[]? content = message?.content;
        test:assertEquals(content, "Test message received! How can I assist you today?");
    }
}

import ballerina/http;
import ballerina/regex;

type WifiPayload readonly & record {|
    string username;
    string email;
    string password;
|};

type WifiPayloadRecord record {|
    readonly string email;
    WifiPayload[] wifiAccounts;
|};

table<WifiPayloadRecord> key(email) wifiAccounts = table [
    {email: "alice@gmail.com", wifiAccounts: [{email: "alice@gmail.com", username: "newuser", password: "newpass"}]},
    {email: "john@gmail.com", wifiAccounts: [{email: "jonh@gmail.com", username: "newGuest", password: "john123"}]}
];

service / on new http:Listener(9090) {

    resource function get guest\-wifi\-accounts/[string ownerEmail]() returns string[] {
        string[] payload = [];
        if (!wifiAccounts.hasKey(ownerEmail)) {
            return payload;
        }
        WifiPayloadRecord wifiRecords = wifiAccounts.get(ownerEmail);
        foreach WifiPayload wifiAccount in wifiRecords.wifiAccounts {
            if (wifiAccount.email == ownerEmail) {
                payload.push(string `${wifiAccount.username}.guestOf.${regex:split(ownerEmail, "@")[0]}`);
            }
        }
        return payload;
    }

    resource function post guest\-wifi\-accounts(@http:Payload WifiPayload wifiRecord) returns string {

        if !(wifiAccounts.hasKey(wifiRecord.email)) {
            wifiAccounts.add({email: wifiRecord.email, wifiAccounts: [wifiRecord]});
        } else {
            WifiPayloadRecord wifiRecords = wifiAccounts.get(wifiRecord.email);
            wifiRecords.wifiAccounts.push(wifiRecord);
        }
        return "Successfully added the wifi account";
    }
}

// Copyright (c) 2021, WSO2 Inc. (http://www.wso2.org) All Rights Reserved.
//
// WSO2 Inc. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/http;
import inventory_service.inventory as inv;
import inventory_service.representations as rep;

listener http:Listener inventoryEP = new (9091,
    secureSocket = {
        key: {
            certFile: "./resources/public.crt",
            keyFile: "./resources/private.key"
        },
        mutualSsl: {
            verifyClient: http:REQUIRE,
            cert: "./resources/public.crt"
        }
    }
);

service /inventory on inventoryEP {
    resource function put decrease(@http:Payload rep:OrderItem[] orderItems) returns rep:InventoryUpdated {
        foreach rep:OrderItem orderItem in orderItems {
            inv:decreaseQty(orderItem.category, orderItem.code, orderItem.qty);
        }
        return {};
    }

    resource function put increase(@http:Payload rep:OrderItem[] orderItems) returns rep:InventoryUpdated {
        foreach rep:OrderItem orderItem in orderItems {
            inv:increaseQty(orderItem.category, orderItem.code, orderItem.qty);
        }
        return {};
    }
}

// Copyright (c) 2021, WSO2 Inc. (http://www.wso2.org) All Rights Reserved.
//
// WSO2 Inc. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/cache;
import ballerina/http;

final http:JwtValidatorConfig config = {
    issuer: "wso2",
    audience: "ballerina",
    signatureConfig: {
        jwksConfig: {
            url: "https://localhost:9445/oauth2/jwks",
            clientConfig: {
                secureSocket: {
                    cert: "./resources/order_service/public.crt"
                }
            },
            cacheConfig: {
                capacity: 10,
                evictionFactor: 0.25,
                evictionPolicy: cache:LRU,
                defaultMaxAge: -1
            }
        }
    },
    cacheConfig: {
        capacity: 10,
        evictionFactor: 0.25,
        evictionPolicy: cache:LRU,
        defaultMaxAge: -1
    }
};

listener http:Listener orderEP = new (9090,
    secureSocket = {
        key: {
            certFile: "./resources/order_service/public.crt",
            keyFile: "./resources/order_service/private.key"
        }
    }
);

isolated service /'order on orderEP {

    @http:ResourceConfig {
        auth: [
            {
                jwtValidatorConfig: config,
                scopes: "add_order"
            }
        ]
    }
    isolated resource function post .(@http:Payload json payload) returns json {
        return payload;
    }
}

// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerinax/ai.agent;

class Data {
    public int age = 12;
}

@agent:Tool
isolated function toolReturningAny(string name) returns any {
    return ();
}

@agent:Tool
isolated function toolReturningInstance(string name) returns Data {
    return new;
}

@agent:Tool
isolated function toolReturningMapOfAny(string name) returns map<any> {
    return {};
}

@agent:Tool
isolated function toolReturningUnionOfAny(string name) returns Data|string {
    return new;
}
